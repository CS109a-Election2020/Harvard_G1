{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to start working on modelling the states without the gdp index. We will try to keep to approach as modular as possible, in case we need to change the selected dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = abs(pd.read_csv('Correlation_matrix.csv'))*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will always take the absolute value of the round of 10 correlation between states. First let us focus on swing states. We will need to select the swing state based on sources **previous to the 2020 election**. Source selected: https://fr.wikipedia.org/wiki/Swing_state#Historique (last modification: 24 october 2020).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big problem will be the way we will construct the validation set. It needs to be from the swing state considered. How can we prevent the creation of a deterministic validation set while imposing such a constraint ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting a list of states and swing states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "swing_states = ['Texas', 'Florida', 'Ohio', 'Georgia', 'North Carolina', 'Arizona', 'Iowa', 'Pennsylvania', 'Michigan', 'Virginia', 'Minnesota', 'Wisconsin', 'Colorado', 'Nevada', 'New Hampshire']\n",
    "all_states = []\n",
    "index_swing_states = []\n",
    "for i, file in enumerate(os.listdir('states/')):\n",
    "    state = file.split('.')[0]\n",
    "    all_states.append(state)\n",
    "    if state in swing_states:\n",
    "        index_swing_states.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct the base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_importance_dfs = {}\n",
    "for index_state in index_swing_states:\n",
    "    data_swing_state = []\n",
    "    for i, state in enumerate(all_states):\n",
    "        data_state = final_df[final_df['State'] == state]\n",
    "        correlation = int(correlation_matrix.iloc[index_state, i])\n",
    "        if correlation > 0:\n",
    "            data_state_importance = pd.concat([data_state for _ in range(correlation)])\n",
    "            if data_state.values.tolist():\n",
    "                try:\n",
    "                    data_swing_state = pd.concat([data_swing_state, data_state_importance])\n",
    "                except:\n",
    "                    data_swing_state = data_state_importance\n",
    "    final_importance_dfs[all_states[index_state]] = data_swing_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.380492</td>\n",
       "      <td>56.679982</td>\n",
       "      <td>53.953977</td>\n",
       "      <td>1</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>53.370</td>\n",
       "      <td>76.757225</td>\n",
       "      <td>52087.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.295363</td>\n",
       "      <td>36.285676</td>\n",
       "      <td>31.824211</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>45.650</td>\n",
       "      <td>76.757225</td>\n",
       "      <td>52087.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.974670</td>\n",
       "      <td>38.698232</td>\n",
       "      <td>43.976458</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>43.010</td>\n",
       "      <td>78.959026</td>\n",
       "      <td>56035.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.175509</td>\n",
       "      <td>41.956795</td>\n",
       "      <td>44.865857</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>37.450</td>\n",
       "      <td>78.959026</td>\n",
       "      <td>56035.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.765363</td>\n",
       "      <td>40.999200</td>\n",
       "      <td>40.518828</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.230</td>\n",
       "      <td>81.841997</td>\n",
       "      <td>60091.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>58.828129</td>\n",
       "      <td>60.276110</td>\n",
       "      <td>60.148473</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.660</td>\n",
       "      <td>5.582234</td>\n",
       "      <td>9154.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57.320477</td>\n",
       "      <td>60.283224</td>\n",
       "      <td>57.678128</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.090</td>\n",
       "      <td>5.985616</td>\n",
       "      <td>10109.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21.309231</td>\n",
       "      <td>22.211773</td>\n",
       "      <td>20.764698</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.180</td>\n",
       "      <td>5.985616</td>\n",
       "      <td>10109.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29.375023</td>\n",
       "      <td>30.842567</td>\n",
       "      <td>32.272552</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3290 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Year  Rep_House_Prop  \\\n",
       "0   56.380492  56.679982  53.953977           1  1988        0.285714   \n",
       "1   33.295363  36.285676  31.824211           0  1988        0.714286   \n",
       "2   42.974670  38.698232  43.976458           0  1992        0.714286   \n",
       "3   42.175509  41.956795  44.865857           1  1992        0.285714   \n",
       "4   40.765363  40.999200  40.518828           0  1996        0.571429   \n",
       "..        ...        ...        ...         ...   ...             ...   \n",
       "9   58.828129  60.276110  60.148473           1  2008        1.000000   \n",
       "10  57.320477  60.283224  57.678128           1  2016        1.000000   \n",
       "11  21.309231  22.211773  20.764698           0  2016        0.000000   \n",
       "12  66.209900  62.336910  66.226278           1  2020        1.000000   \n",
       "13  29.375023  30.842567  32.272552           0  2020        0.000000   \n",
       "\n",
       "      State  Result  rep_loyalty  popular_vote_percentage    density       RDI  \n",
       "0   Alabama       1          0.8                   53.370  76.757225  52087.05  \n",
       "1   Alabama       0          0.8                   45.650  76.757225  52087.05  \n",
       "2   Alabama       0          0.9                   43.010  78.959026  56035.35  \n",
       "3   Alabama       1          0.9                   37.450  78.959026  56035.35  \n",
       "4   Alabama       0          1.0                   49.230  81.841997  60091.59  \n",
       "..      ...     ...          ...                      ...        ...       ...  \n",
       "9   Wyoming       1          1.0                   45.660   5.582234   9154.98  \n",
       "10  Wyoming       1          1.0                   46.090   5.985616  10109.96  \n",
       "11  Wyoming       0          1.0                   48.180   5.985616  10109.96  \n",
       "12  Wyoming       1          1.0                   46.525   5.796735  10593.19  \n",
       "13  Wyoming       0          1.0                   52.460   5.796735  10593.19  \n",
       "\n",
       "[3290 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_importance_dfs['Texas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelling part : swing state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the swing state you wish to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "swing_state = 'Texas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(final_importance_df, swing_state):\n",
    "    swing_state_data = final_importance_df[final_importance_df['State'] == swing_state]\n",
    "    data_test = swing_state_data[swing_state_data['Year'] == 2020].iloc[:2, :]\n",
    "    X_test, y_test = data_test.drop('Result', axis=1), data_test['Result']\n",
    "    swing_state_data = swing_state_data[swing_state_data['Year'] != 2020]\n",
    "    X_train, X_val = train_test_split(swing_state_data, train_size = 0.7)\n",
    "    X_val, y_val = X_val.drop('Result', axis=1), X_val['Result']\n",
    "    X_values = pd.concat((final_importance_df[final_importance_df['State'] != swing_state], X_train))\n",
    "    X_values = X_values.sample(frac=1, replace=False) \n",
    "    X, y = X_values.drop('Result', axis=1), X_values['Result']\n",
    "    return X, X_val, X_test, y, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split(final_importance_dfs[swing_state], swing_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(X_train, X_val, y_train, y_val, models, scaler=True, Poly=1, PCA_comp=0):\n",
    "    # pca is the total variance explained required. 0 means that we do not want to perform pca\n",
    "    if 'State' in X_train.columns:\n",
    "        X_train = X_train.drop('State', axis=1)\n",
    "        X_val = X_val.drop('State', axis=1)\n",
    "    if 'Year' in X_train.columns:\n",
    "        X_train = X_train.drop('Year', axis=1)\n",
    "        X_val = X_val.drop('Year', axis=1)\n",
    "    if PCA_comp > 0 and not scaler:\n",
    "        raise ValueError('We must normalize before performing PCA')\n",
    "    # if PCA_comp > 1 and Poly > 1:\n",
    "        # print('Performing Polynomial Transformation on top of PCA ...')\n",
    "    if scaler:\n",
    "        # print('Normalizing the data ...')\n",
    "        scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "    if PCA_comp > 0:\n",
    "        # print('Performing PCA ...')\n",
    "        pca = PCA(n_components=X_train.shape[1])\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_val = pca.transform(X_val)\n",
    "        total_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        for i, variance in enumerate(total_variance):\n",
    "            if variance > PCA_comp:\n",
    "                break \n",
    "        X_train, X_val = X_train[:, :i+1], X_val[:, :i+1]\n",
    "    if Poly > 1:\n",
    "        # print('Polynomial transformation ...')\n",
    "        poly_features = PolynomialFeatures(degree=Poly, include_bias=False).fit(X_train)\n",
    "        X_train = poly_features.transform(X_train)\n",
    "        X_val = poly_features.transform(X_val)\n",
    "    f1s = []\n",
    "    accuracies = []\n",
    "    auc = []\n",
    "    # print('Modelling and gathering the predictions ..')\n",
    "    for i, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_val)\n",
    "        accuracies.append(accuracy_score(y_val, predictions))\n",
    "        f1s.append(f1_score(y_val, predictions))\n",
    "        auc.append(roc_auc_score(y_val, predictions))\n",
    "    return models, f1s, accuracies, auc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, f1s, accuracies, auc, predictions = modelling(X_train, X_val, y_train, y_val, models=[LogisticRegression()], scaler=True, Poly=2, PCA_comp=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegression(), DecisionTreeClassifier()]\n",
    "F1, acc, auc = [], [], []\n",
    "for state in swing_states:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split(final_importance_dfs[state], state)\n",
    "    models, f1s, accuracies, aucs, predictions = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "    F1.append(f1s)\n",
    "    acc.append(accuracies)\n",
    "    auc.append(aucs)\n",
    "F1_mean, acc_mean, auc_mean = np.mean(np.array(f1s), axis=0), np.mean(np.array(acc), axis=0), np.mean(np.array(auc), axis=0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

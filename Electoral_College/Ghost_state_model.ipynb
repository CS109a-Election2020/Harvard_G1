{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will start by importing the data\n",
    "\n",
    "In the notebook we work with the ghost state approach. Therefore our dataframe will consist of all the data for the previous elections for all states, concatenated in a single dataframe.\n",
    "\n",
    "We will begin by reading the .csv files for the different states and concatenate them into a single dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the base dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Year  Rep_House_Prop    State  \\\n",
       "0  51.797513  51.909088  51.657848           1  1988        0.200000  Georgia   \n",
       "1  37.538311  38.412820  37.298773           0  1988        0.800000  Georgia   \n",
       "2  41.056567  39.511030  39.876493           0  1992        0.900000  Georgia   \n",
       "3  35.651345  35.969860  38.271617           1  1992        0.100000  Georgia   \n",
       "4  46.538179  44.819709  45.546053           0  1996        0.636364  Georgia   \n",
       "\n",
       "   Result  rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "0       1          0.5                    53.37  106.259013   93086.74  \n",
       "1       0          0.5                    45.65  106.259013   93086.74  \n",
       "2       1          0.7                    43.01  113.717367  100758.19  \n",
       "3       0          0.7                    37.45  113.717367  100758.19  \n",
       "4       0          0.5                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(base_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test data (test data 2020 observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = base_df[base_df.Year != 2020]\n",
    "test_df = base_df[base_df.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Year  Rep_House_Prop    State  \\\n",
       "0  51.797513  51.909088  51.657848           1  1988        0.200000  Georgia   \n",
       "1  37.538311  38.412820  37.298773           0  1988        0.800000  Georgia   \n",
       "2  41.056567  39.511030  39.876493           0  1992        0.900000  Georgia   \n",
       "3  35.651345  35.969860  38.271617           1  1992        0.100000  Georgia   \n",
       "4  46.538179  44.819709  45.546053           0  1996        0.636364  Georgia   \n",
       "\n",
       "   Result  rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "0       1          0.5                    53.37  106.259013   93086.74  \n",
       "1       0          0.5                    45.65  106.259013   93086.74  \n",
       "2       1          0.7                    43.01  113.717367  100758.19  \n",
       "3       0          0.7                    37.45  113.717367  100758.19  \n",
       "4       0          0.5                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>State</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47.604381</td>\n",
       "      <td>48.407587</td>\n",
       "      <td>45.906192</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.855849</td>\n",
       "      <td>48.848577</td>\n",
       "      <td>47.920197</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52.460</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Year  Rep_House_Prop  \\\n",
       "16  46.657139  47.298710  47.170450           1  2020        0.714286   \n",
       "17  47.604381  48.407587  45.906192           0  2020        0.285714   \n",
       "16  46.311941  47.027987  46.593060           1  2020        0.785714   \n",
       "17  48.855849  48.848577  47.920197           0  2020        0.214286   \n",
       "12  66.209900  62.336910  66.226278           1  2020        1.000000   \n",
       "\n",
       "             State  Result  rep_loyalty  popular_vote_percentage     density  \\\n",
       "16         Georgia       1          1.0                   46.525  180.617755   \n",
       "17         Georgia       0          1.0                   52.460  180.617755   \n",
       "16  North Carolina       1          0.8                   46.525  197.170250   \n",
       "17  North Carolina       0          0.8                   52.460  197.170250   \n",
       "12         Wyoming       1          1.0                   46.525    5.796735   \n",
       "\n",
       "          RDI  \n",
       "16  163524.50  \n",
       "17  163524.50  \n",
       "16  161835.04  \n",
       "17  161835.04  \n",
       "12   10593.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Rep_House_Prop  Result  \\\n",
       "0  51.797513  51.909088  51.657848           1        0.200000       1   \n",
       "1  37.538311  38.412820  37.298773           0        0.800000       0   \n",
       "2  41.056567  39.511030  39.876493           0        0.900000       1   \n",
       "3  35.651345  35.969860  38.271617           1        0.100000       0   \n",
       "4  46.538179  44.819709  45.546053           0        0.636364       0   \n",
       "\n",
       "   rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "0          0.5                    53.37  106.259013   93086.74  \n",
       "1          0.5                    45.65  106.259013   93086.74  \n",
       "2          0.7                    43.01  113.717367  100758.19  \n",
       "3          0.7                    37.45  113.717367  100758.19  \n",
       "4          0.5                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>Result</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47.604381</td>\n",
       "      <td>48.407587</td>\n",
       "      <td>45.906192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.855849</td>\n",
       "      <td>48.848577</td>\n",
       "      <td>47.920197</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52.460</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Rep_House_Prop  Result  \\\n",
       "16  46.657139  47.298710  47.170450           1        0.714286       1   \n",
       "17  47.604381  48.407587  45.906192           0        0.285714       0   \n",
       "16  46.311941  47.027987  46.593060           1        0.785714       1   \n",
       "17  48.855849  48.848577  47.920197           0        0.214286       0   \n",
       "12  66.209900  62.336910  66.226278           1        1.000000       1   \n",
       "\n",
       "    rep_loyalty  popular_vote_percentage     density        RDI  \n",
       "16          1.0                   46.525  180.617755  163524.50  \n",
       "17          1.0                   52.460  180.617755  163524.50  \n",
       "16          0.8                   46.525  197.170250  161835.04  \n",
       "17          0.8                   52.460  197.170250  161835.04  \n",
       "12          1.0                   46.525    5.796735   10593.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, train_size = 0.8, random_state=56)\n",
    "X_train = train.drop('Result', axis=1)#.values\n",
    "y_train = train['Result']#.values\n",
    "X_val = val.drop('Result', axis=1)#.values\n",
    "y_val = val['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(X_train, X_val, y_train, y_val, models, scaler=True, Poly=1, PCA_comp=0):\n",
    "    # pca is the total variance explained required. 0 means that we do not want to perform pca\n",
    "    if 'State' in X_train.columns:\n",
    "        X_train = X_train.drop('State', axis=1)\n",
    "        X_val = X_val.drop('State', axis=1)\n",
    "    if 'Year' in X_train.columns:\n",
    "        X_train = X_train.drop('Year', axis=1)\n",
    "        X_val = X_val.drop('Year', axis=1)\n",
    "    if PCA_comp > 0 and not scaler:\n",
    "        raise ValueError('We must normalize before performing PCA')\n",
    "    if scaler:\n",
    "        # print('Normalizing the data ...')\n",
    "        scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "    if PCA_comp > 0:\n",
    "        # print('Performing PCA ...')\n",
    "        pca = PCA(n_components=X_train.shape[1])\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_val = pca.transform(X_val)\n",
    "        total_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        for i, variance in enumerate(total_variance):\n",
    "            if variance > PCA_comp:\n",
    "                break \n",
    "        X_train, X_val = X_train[:, :i+1], X_val[:, :i+1]\n",
    "    if Poly > 1:\n",
    "        # print('Polynomial transformation ...')\n",
    "        poly_features = PolynomialFeatures(degree=Poly, include_bias=False).fit(X_train)\n",
    "        X_train = poly_features.transform(X_train)\n",
    "        X_val = poly_features.transform(X_val)\n",
    "    accuracies = []\n",
    "    auc = []\n",
    "    # print('Modelling and gathering the predictions ..')\n",
    "    for i, model in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_val)\n",
    "        accuracies.append(accuracy_score(y_val, predictions))\n",
    "        try:\n",
    "            auc.append(roc_auc_score(y_val, predictions))\n",
    "        except ValueError:\n",
    "            auc.append(1)\n",
    "    return models, accuracies, auc, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-7fe71a2d006a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodels_considered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saga'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maucs_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels_considered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCA_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga')]\n",
    "models_11, accuracies_11, aucs_11, predictions_11 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0}: {1}\".format(models_11[0], accuracies_11[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_12, accuracies_12, aucs_12, predictions_12 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga'): 0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}: {1}\".format(models_12[0], accuracies_12[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860759493670886\n",
      "0.96\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=2, include_bias=False).fit(X_train)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train1 = scaler.transform(X_train)\n",
    "X_val1 = scaler.transform(X_val)\n",
    "X_test1 = scaler.transform(X_test)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train1)\n",
    "X_val_poly = poly_features.transform(X_val1)\n",
    "X_test_poly = poly_features.transform(X_test1)\n",
    "model = LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga').fit(X_train_poly, y_train)\n",
    "print(accuracy_score(y_val, model.predict(X_val_poly)))\n",
    "print(accuracy_score(y_test, model.predict(X_test_poly)))\n",
    "print(model.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_13, accuracies_13, aucs_13, predictions_13 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga'): 0.8860759493670886\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}: {1}\".format(models_13[0], accuracies_13[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga')]\n",
    "models_14, accuracies_14, aucs_14, predictions_14 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga'): 0.7911392405063291\n"
     ]
    }
   ],
   "source": [
    "print(\"{0}: {1}\".format(models_14[0], accuracies_14[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree Classifier\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8734177215189873\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.879746835443038\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8987341772151899\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8734177215189873\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.879746835443038\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8860759493670886\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_21, accuracies_21, aucs_21, predictions_21 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_21[0], accuracies_21[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.810126582278481\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8670886075949367\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8544303797468354\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8734177215189873\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8670886075949367\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_22, accuracies_22, aucs_22, predictions_22 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_22[0], accuracies_22[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.7911392405063291\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8164556962025317\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8227848101265823\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8354430379746836\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8924050632911392\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8607594936708861\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8544303797468354\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_23, accuracies_23, aucs_23, predictions_23 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_23[0], accuracies_23[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.7848101265822784\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.7848101265822784\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8227848101265823\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8481012658227848\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8227848101265823\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8291139240506329\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8544303797468354\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_24, accuracies_24, aucs_24, predictions_24 = modelling(X_train, X_val, y_train, y_val, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_24[0], accuracies_24[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1728 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 530 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 880 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1330 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1880 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2530 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3280 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators_over = [50, 90, 110, 150, 200, 300, 400, 500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth_over = [int(x) for x in np.linspace(1, 10, num=6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 3]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators_over,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth_over,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "\n",
    "               }\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "random_model = GridSearchCV(estimator=rf, param_grid=random_grid, verbose=1,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=-1, return_train_score=True, cv=2, refit=True)\n",
    "random_model.fit(X_train, y_train)\n",
    "dict_rf = cross_validate(RandomForestClassifier(**random_model.best_params_), X_train, y_train, cv=9, scoring='accuracy')\n",
    "rf_initial_accuracy = np.mean(dict_rf['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 10,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 110}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8904761904761904"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_initial_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  1.0 the CV score is  0.8746031746031746\n",
      "{'learning_rate': 0.1, 'n_estimators': 64}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  2.0 the CV score is  0.8285714285714286\n",
      "{'learning_rate': 0.01, 'n_estimators': 159}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  3.0 the CV score is  0.8428571428571429\n",
      "{'learning_rate': 0.1, 'n_estimators': 23}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  4.0 the CV score is  0.8746031746031746\n",
      "{'learning_rate': 1, 'n_estimators': 91}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  5.0 the CV score is  0.8682539682539683\n",
      "{'learning_rate': 1, 'n_estimators': 186}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  6.0 the CV score is  0.8253968253968255\n",
      "{'learning_rate': 0.01, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  7.0 the CV score is  0.8714285714285714\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 150 | elapsed:    1.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  8.0 the CV score is  0.8714285714285714\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  9.0 the CV score is  0.8380952380952381\n",
      "{'learning_rate': 0.01, 'n_estimators': 172}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  10.0 the CV score is  0.8253968253968255\n",
      "{'learning_rate': 0.001, 'n_estimators': 91}\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Base Estimator\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 15)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "scores = []\n",
    "params_scaled = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = depth))\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2, scoring = 'accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    dict_boosting = cross_validate(AdaBoostClassifier(**grid_search.best_params_), X_train, y_train, cv=9, scoring='accuracy', n_jobs=-1)\n",
    "    boosting_accuracy = np.mean(dict_boosting['test_score'])\n",
    "    print('With components ', depth, 'the CV score is ', boosting_accuracy)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-do everything with cross-validation only, no splitting in the traning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[base_df.Year != 2020]\n",
    "test_df = df[base_df.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Result', axis=1)#.values\n",
    "y_train = train_df['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(X_train, y_train, models, scaler=True, Poly=1, PCA_comp=0):\n",
    "    # pca is the total variance explained required. 0 means that we do not want to perform pca\n",
    "    if 'State' in X_train.columns:\n",
    "        X_train = X_train.drop('State', axis=1)\n",
    "    if 'Year' in X_train.columns:\n",
    "        X_train = X_train.drop('Year', axis=1)\n",
    "    if PCA_comp > 0 and not scaler:\n",
    "        raise ValueError('We must normalize before performing PCA')\n",
    "    if scaler:\n",
    "        # print('Normalizing the data ...')\n",
    "        scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "    if PCA_comp > 0:\n",
    "        # print('Performing PCA ...')\n",
    "        pca = PCA(n_components=X_train.shape[1])\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        total_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        for i, variance in enumerate(total_variance):\n",
    "            if variance > PCA_comp:\n",
    "                break \n",
    "        X_train = X_train[:, :i+1]\n",
    "    if Poly > 1:\n",
    "        # print('Polynomial transformation ...')\n",
    "        poly_features = PolynomialFeatures(degree=Poly, include_bias=False).fit(X_train)\n",
    "        X_train = poly_features.transform(X_train)\n",
    "    model_cv_accuracies = []\n",
    "    # print('Modelling and gathering the predictions ..')\n",
    "    for i, model in enumerate(models):\n",
    "        model_cv = cross_validate(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        model_cv_accuracy = np.mean(model_cv['test_score'])\n",
    "        model_cv_accuracies.append(model_cv_accuracy)\n",
    "    return models, model_cv_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8705877610255583]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=1000, penalty='l1', solver='saga')]\n",
    "models_11, cv_accuracies_11 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "print(cv_accuracies_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.879464645650246]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_12, cv_accuracies_12 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "print(cv_accuracies_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879464645650246\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=2, include_bias=False).fit(X_train)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train1 = scaler.transform(X_train)\n",
    "X_test1 = scaler.transform(X_test)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train1)\n",
    "X_test_poly = poly_features.transform(X_test1)\n",
    "model = LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')\n",
    "print(np.mean(cross_validate(model, X_train_poly, y_train, cv=5, scoring='accuracy')['test_score']))\n",
    "model.fit(X_train_poly, y_train)\n",
    "print(model.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8476900749818593]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_13, cv_accuracies_13 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "print(cv_accuracies_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7640248327017657]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_14, cv_accuracies_14 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "print(cv_accuracies_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree Classifier\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8262033379021204\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8262033379021204\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8604611787470773\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8693058131097317\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8451664919777473\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8489881480287028\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.850253970813513\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8527775538176247\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8363137950495847\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_21, cv_accuracies_21 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_21[0], cv_accuracies_21[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8300088688220593\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8300088688220593\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8414254615818753\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8248730145932436\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8286543578166572\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.843916794323954\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8489639603321777\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8400951382729985\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8349995968717245\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_22, cv_accuracies_22 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_22[0], cv_accuracies_22[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.7804402160767556\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8032895267274046\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8312021285172942\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8020075788115779\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8209787954527131\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.815947754575506\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8096025155204385\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8184874627106347\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.815947754575506\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_23, cv_accuracies_23 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_23[0], cv_accuracies_23[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.765339030879626\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.7589292913004918\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.7881157784406998\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8121664113520921\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8147383697492543\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8147061194872208\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.828670482947674\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8197774731919697\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8184794001451262\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_24, cv_accuracies_24 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_24[0], cv_accuracies_24[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1728 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators_over = [50, 90, 110, 150, 200, 300, 400, 500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth_over = [int(x) for x in np.linspace(1, 10, num=6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 3]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators_over,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth_over,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "\n",
    "               }\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "random_model = GridSearchCV(estimator=rf, param_grid=random_grid, verbose=1,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=-1, return_train_score=True, cv=2, refit=True)\n",
    "random_model.fit(X_train, y_train)\n",
    "dict_rf = cross_validate(RandomForestClassifier(**random_model.best_params_), X_train, y_train, cv=9, scoring='accuracy')\n",
    "rf_initial_accuracy = np.mean(dict_rf['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "0.8934169278996866\n"
     ]
    }
   ],
   "source": [
    "print(random_model.best_params_)\n",
    "print(rf_initial_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(bootstrap = True, max_depth = 8, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 50)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  1.0 the CV score is  0.8871037965865551\n",
      "{'learning_rate': 1, 'n_estimators': 132}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  2.0 the CV score is  0.8503715314060141\n",
      "{'learning_rate': 0.1, 'n_estimators': 23}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  3.0 the CV score is  0.8377017299431092\n",
      "{'learning_rate': 0.01, 'n_estimators': 145}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  4.0 the CV score is  0.8377017299431092\n",
      "{'learning_rate': 0.01, 'n_estimators': 118}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  5.0 the CV score is  0.8694270289097874\n",
      "{'learning_rate': 0.1, 'n_estimators': 37}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  6.0 the CV score is  0.8858556832694764\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  7.0 the CV score is  0.8921978404737025\n",
      "{'learning_rate': 1, 'n_estimators': 105}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  8.0 the CV score is  0.8807616393823291\n",
      "{'learning_rate': 1, 'n_estimators': 37}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  9.0 the CV score is  0.8845930570068501\n",
      "{'learning_rate': 0.1, 'n_estimators': 105}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  10.0 the CV score is  0.8402269824683617\n",
      "{'learning_rate': 0.01, 'n_estimators': 159}\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Base Estimator\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 15)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "scores = []\n",
    "params_scaled = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = depth))\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2, scoring = 'accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    dict_boosting = cross_validate(AdaBoostClassifier(**grid_search.best_params_), X_train, y_train, cv=9, scoring='accuracy', n_jobs=-1)\n",
    "    boosting_accuracy = np.mean(dict_boosting['test_score'])\n",
    "    print('With components ', depth, 'the CV score is ', boosting_accuracy)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 7), learning_rate = 1, n_estimators = 105).fit(X_train, y_train)\n",
    "ada.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[base_df.Year != 2020]\n",
    "test_df = df[base_df.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Result', axis=1)#.values\n",
    "y_train = train_df['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.538311</td>\n",
       "      <td>38.412820</td>\n",
       "      <td>37.298773</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>45.65</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.056567</td>\n",
       "      <td>39.511030</td>\n",
       "      <td>39.876493</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>43.01</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.538179</td>\n",
       "      <td>44.819709</td>\n",
       "      <td>45.546053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.23</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Rep_House_Prop  rep_loyalty  \\\n",
       "0  51.797513  51.909088  51.657848           1        0.200000          0.5   \n",
       "1  37.538311  38.412820  37.298773           0        0.800000          0.5   \n",
       "2  41.056567  39.511030  39.876493           0        0.900000          0.7   \n",
       "3  35.651345  35.969860  38.271617           1        0.100000          0.7   \n",
       "4  46.538179  44.819709  45.546053           0        0.636364          0.5   \n",
       "\n",
       "   popular_vote_percentage     density        RDI  \n",
       "0                    53.37  106.259013   93086.74  \n",
       "1                    45.65  106.259013   93086.74  \n",
       "2                    43.01  113.717367  100758.19  \n",
       "3                    37.45  113.717367  100758.19  \n",
       "4                    49.23  123.352989  108448.50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47.604381</td>\n",
       "      <td>48.407587</td>\n",
       "      <td>45.906192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.460</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.855849</td>\n",
       "      <td>48.848577</td>\n",
       "      <td>47.920197</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.8</td>\n",
       "      <td>52.460</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Rep_House_Prop  rep_loyalty  \\\n",
       "16  46.657139  47.298710  47.170450           1        0.714286          1.0   \n",
       "17  47.604381  48.407587  45.906192           0        0.285714          1.0   \n",
       "16  46.311941  47.027987  46.593060           1        0.785714          0.8   \n",
       "17  48.855849  48.848577  47.920197           0        0.214286          0.8   \n",
       "12  66.209900  62.336910  66.226278           1        1.000000          1.0   \n",
       "\n",
       "    popular_vote_percentage     density        RDI  \n",
       "16                   46.525  180.617755  163524.50  \n",
       "17                   52.460  180.617755  163524.50  \n",
       "16                   46.525  197.170250  161835.04  \n",
       "17                   52.460  197.170250  161835.04  \n",
       "12                   46.525    5.796735   10593.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Defining an Early Stopping Condition\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 50)\n",
    "regularizer = 0.00015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 158 samples\n",
      "Epoch 1/50\n",
      "630/630 - 1s - loss: 0.9394 - accuracy: 0.4952 - val_loss: 0.8899 - val_accuracy: 0.4747\n",
      "Epoch 2/50\n",
      "630/630 - 0s - loss: 0.8626 - accuracy: 0.4825 - val_loss: 0.7851 - val_accuracy: 0.4557\n",
      "Epoch 3/50\n",
      "630/630 - 0s - loss: 0.7792 - accuracy: 0.4667 - val_loss: 0.7226 - val_accuracy: 0.4747\n",
      "Epoch 4/50\n",
      "630/630 - 0s - loss: 0.7192 - accuracy: 0.4889 - val_loss: 0.6826 - val_accuracy: 0.5316\n",
      "Epoch 5/50\n",
      "630/630 - 0s - loss: 0.7033 - accuracy: 0.5111 - val_loss: 0.6488 - val_accuracy: 0.6329\n",
      "Epoch 6/50\n",
      "630/630 - 0s - loss: 0.6645 - accuracy: 0.5698 - val_loss: 0.6058 - val_accuracy: 0.7152\n",
      "Epoch 7/50\n",
      "630/630 - 0s - loss: 0.6276 - accuracy: 0.6190 - val_loss: 0.5557 - val_accuracy: 0.7405\n",
      "Epoch 8/50\n",
      "630/630 - 0s - loss: 0.6025 - accuracy: 0.6429 - val_loss: 0.5150 - val_accuracy: 0.7785\n",
      "Epoch 9/50\n",
      "630/630 - 0s - loss: 0.5643 - accuracy: 0.6746 - val_loss: 0.4782 - val_accuracy: 0.7848\n",
      "Epoch 10/50\n",
      "630/630 - 0s - loss: 0.5476 - accuracy: 0.7032 - val_loss: 0.4408 - val_accuracy: 0.7911\n",
      "Epoch 11/50\n",
      "630/630 - 0s - loss: 0.5415 - accuracy: 0.7000 - val_loss: 0.4188 - val_accuracy: 0.7975\n",
      "Epoch 12/50\n",
      "630/630 - 0s - loss: 0.5129 - accuracy: 0.7476 - val_loss: 0.4008 - val_accuracy: 0.8165\n",
      "Epoch 13/50\n",
      "630/630 - 0s - loss: 0.5221 - accuracy: 0.7333 - val_loss: 0.3885 - val_accuracy: 0.8165\n",
      "Epoch 14/50\n",
      "630/630 - 0s - loss: 0.5158 - accuracy: 0.7476 - val_loss: 0.3814 - val_accuracy: 0.8165\n",
      "Epoch 15/50\n",
      "630/630 - 0s - loss: 0.4940 - accuracy: 0.7524 - val_loss: 0.3658 - val_accuracy: 0.8291\n",
      "Epoch 16/50\n",
      "630/630 - 0s - loss: 0.4923 - accuracy: 0.7698 - val_loss: 0.3552 - val_accuracy: 0.8354\n",
      "Epoch 17/50\n",
      "630/630 - 0s - loss: 0.4720 - accuracy: 0.7841 - val_loss: 0.3471 - val_accuracy: 0.8418\n",
      "Epoch 18/50\n",
      "630/630 - 0s - loss: 0.4701 - accuracy: 0.7810 - val_loss: 0.3343 - val_accuracy: 0.8481\n",
      "Epoch 19/50\n",
      "630/630 - 0s - loss: 0.4675 - accuracy: 0.7889 - val_loss: 0.3272 - val_accuracy: 0.8481\n",
      "Epoch 20/50\n",
      "630/630 - 0s - loss: 0.4562 - accuracy: 0.8063 - val_loss: 0.3172 - val_accuracy: 0.8544\n",
      "Epoch 21/50\n",
      "630/630 - 0s - loss: 0.4602 - accuracy: 0.7794 - val_loss: 0.3162 - val_accuracy: 0.8544\n",
      "Epoch 22/50\n",
      "630/630 - 0s - loss: 0.4601 - accuracy: 0.8095 - val_loss: 0.3144 - val_accuracy: 0.8481\n",
      "Epoch 23/50\n",
      "630/630 - 0s - loss: 0.4528 - accuracy: 0.8222 - val_loss: 0.3110 - val_accuracy: 0.8481\n",
      "Epoch 24/50\n",
      "630/630 - 0s - loss: 0.4352 - accuracy: 0.8175 - val_loss: 0.3054 - val_accuracy: 0.8544\n",
      "Epoch 25/50\n",
      "630/630 - 0s - loss: 0.4322 - accuracy: 0.8254 - val_loss: 0.2991 - val_accuracy: 0.8671\n",
      "Epoch 26/50\n",
      "630/630 - 0s - loss: 0.4369 - accuracy: 0.8222 - val_loss: 0.2935 - val_accuracy: 0.8608\n",
      "Epoch 27/50\n",
      "630/630 - 0s - loss: 0.4211 - accuracy: 0.8175 - val_loss: 0.2893 - val_accuracy: 0.8734\n",
      "Epoch 28/50\n",
      "630/630 - 0s - loss: 0.4307 - accuracy: 0.8190 - val_loss: 0.2909 - val_accuracy: 0.8797\n",
      "Epoch 29/50\n",
      "630/630 - 0s - loss: 0.4213 - accuracy: 0.8222 - val_loss: 0.2854 - val_accuracy: 0.8734\n",
      "Epoch 30/50\n",
      "630/630 - 0s - loss: 0.4061 - accuracy: 0.8333 - val_loss: 0.2806 - val_accuracy: 0.8797\n",
      "Epoch 31/50\n",
      "630/630 - 0s - loss: 0.4305 - accuracy: 0.8270 - val_loss: 0.2780 - val_accuracy: 0.8797\n",
      "Epoch 32/50\n",
      "630/630 - 0s - loss: 0.4380 - accuracy: 0.8127 - val_loss: 0.2797 - val_accuracy: 0.8797\n",
      "Epoch 33/50\n",
      "630/630 - 0s - loss: 0.4552 - accuracy: 0.8254 - val_loss: 0.2775 - val_accuracy: 0.8861\n",
      "Epoch 34/50\n",
      "630/630 - 0s - loss: 0.4051 - accuracy: 0.8349 - val_loss: 0.2762 - val_accuracy: 0.8861\n",
      "Epoch 35/50\n",
      "630/630 - 0s - loss: 0.4189 - accuracy: 0.8238 - val_loss: 0.2722 - val_accuracy: 0.8924\n",
      "Epoch 36/50\n",
      "630/630 - 0s - loss: 0.4089 - accuracy: 0.8397 - val_loss: 0.2651 - val_accuracy: 0.8924\n",
      "Epoch 37/50\n",
      "630/630 - 0s - loss: 0.3916 - accuracy: 0.8429 - val_loss: 0.2633 - val_accuracy: 0.8861\n",
      "Epoch 38/50\n",
      "630/630 - 0s - loss: 0.4184 - accuracy: 0.8222 - val_loss: 0.2606 - val_accuracy: 0.8861\n",
      "Epoch 39/50\n",
      "630/630 - 0s - loss: 0.4075 - accuracy: 0.8460 - val_loss: 0.2584 - val_accuracy: 0.8924\n",
      "Epoch 40/50\n",
      "630/630 - 0s - loss: 0.3973 - accuracy: 0.8444 - val_loss: 0.2542 - val_accuracy: 0.8924\n",
      "Epoch 41/50\n",
      "630/630 - 0s - loss: 0.3958 - accuracy: 0.8349 - val_loss: 0.2526 - val_accuracy: 0.8987\n",
      "Epoch 42/50\n",
      "630/630 - 0s - loss: 0.3950 - accuracy: 0.8365 - val_loss: 0.2493 - val_accuracy: 0.8987\n",
      "Epoch 43/50\n",
      "630/630 - 0s - loss: 0.3635 - accuracy: 0.8540 - val_loss: 0.2468 - val_accuracy: 0.8924\n",
      "Epoch 44/50\n",
      "630/630 - 0s - loss: 0.3892 - accuracy: 0.8429 - val_loss: 0.2448 - val_accuracy: 0.8987\n",
      "Epoch 45/50\n",
      "630/630 - 0s - loss: 0.3968 - accuracy: 0.8476 - val_loss: 0.2418 - val_accuracy: 0.8987\n",
      "Epoch 46/50\n",
      "630/630 - 0s - loss: 0.3861 - accuracy: 0.8397 - val_loss: 0.2369 - val_accuracy: 0.8987\n",
      "Epoch 47/50\n",
      "630/630 - 0s - loss: 0.3928 - accuracy: 0.8508 - val_loss: 0.2357 - val_accuracy: 0.8987\n",
      "Epoch 48/50\n",
      "630/630 - 0s - loss: 0.3999 - accuracy: 0.8381 - val_loss: 0.2376 - val_accuracy: 0.8987\n",
      "Epoch 49/50\n",
      "630/630 - 0s - loss: 0.4091 - accuracy: 0.8333 - val_loss: 0.2390 - val_accuracy: 0.9051\n",
      "Epoch 50/50\n",
      "630/630 - 0s - loss: 0.4016 - accuracy: 0.8365 - val_loss: 0.2389 - val_accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(name = 'ANN')\n",
    "\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu', input_shape=(9,), name='hidden1', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu', name='hidden2', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(4, activation='relu', name='hidden3', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=2, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1 - 0s - loss: 0.1440 - accuracy: 0.9300\n",
      "Test accuracy=0.9300000071525574\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy={test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 551 samples, validate on 237 samples\n",
      "Epoch 1/100\n",
      "551/551 [==============================] - 1s 2ms/sample - loss: 0.6569 - accuracy: 0.6388 - val_loss: 0.5806 - val_accuracy: 0.7173\n",
      "Epoch 2/100\n",
      "551/551 [==============================] - 0s 180us/sample - loss: 0.6185 - accuracy: 0.6733 - val_loss: 0.5525 - val_accuracy: 0.7553\n",
      "Epoch 3/100\n",
      "551/551 [==============================] - 0s 149us/sample - loss: 0.5883 - accuracy: 0.7060 - val_loss: 0.5285 - val_accuracy: 0.7848\n",
      "Epoch 4/100\n",
      "551/551 [==============================] - 0s 155us/sample - loss: 0.5619 - accuracy: 0.7332 - val_loss: 0.5084 - val_accuracy: 0.7932\n",
      "Epoch 5/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.5401 - accuracy: 0.7586 - val_loss: 0.4910 - val_accuracy: 0.7975\n",
      "Epoch 6/100\n",
      "551/551 [==============================] - 0s 149us/sample - loss: 0.5210 - accuracy: 0.7822 - val_loss: 0.4752 - val_accuracy: 0.8059\n",
      "Epoch 7/100\n",
      "551/551 [==============================] - 0s 140us/sample - loss: 0.5031 - accuracy: 0.7931 - val_loss: 0.4598 - val_accuracy: 0.8143\n",
      "Epoch 8/100\n",
      "551/551 [==============================] - 0s 145us/sample - loss: 0.4857 - accuracy: 0.7895 - val_loss: 0.4455 - val_accuracy: 0.8101\n",
      "Epoch 9/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.4695 - accuracy: 0.8022 - val_loss: 0.4321 - val_accuracy: 0.8186\n",
      "Epoch 10/100\n",
      "551/551 [==============================] - 0s 150us/sample - loss: 0.4536 - accuracy: 0.8094 - val_loss: 0.4191 - val_accuracy: 0.8143\n",
      "Epoch 11/100\n",
      "551/551 [==============================] - 0s 142us/sample - loss: 0.4382 - accuracy: 0.8330 - val_loss: 0.4054 - val_accuracy: 0.8101\n",
      "Epoch 12/100\n",
      "551/551 [==============================] - 0s 151us/sample - loss: 0.4241 - accuracy: 0.8385 - val_loss: 0.3924 - val_accuracy: 0.8186\n",
      "Epoch 13/100\n",
      "551/551 [==============================] - 0s 144us/sample - loss: 0.4105 - accuracy: 0.8403 - val_loss: 0.3812 - val_accuracy: 0.8228\n",
      "Epoch 14/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.3988 - accuracy: 0.8421 - val_loss: 0.3706 - val_accuracy: 0.8228\n",
      "Epoch 15/100\n",
      "551/551 [==============================] - 0s 148us/sample - loss: 0.3882 - accuracy: 0.8421 - val_loss: 0.3615 - val_accuracy: 0.8186\n",
      "Epoch 16/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.3784 - accuracy: 0.8457 - val_loss: 0.3532 - val_accuracy: 0.8312\n",
      "Epoch 17/100\n",
      "551/551 [==============================] - 0s 143us/sample - loss: 0.3699 - accuracy: 0.8439 - val_loss: 0.3451 - val_accuracy: 0.8354\n",
      "Epoch 18/100\n",
      "551/551 [==============================] - 0s 140us/sample - loss: 0.3619 - accuracy: 0.8512 - val_loss: 0.3391 - val_accuracy: 0.8439\n",
      "Epoch 19/100\n",
      "551/551 [==============================] - 0s 154us/sample - loss: 0.3551 - accuracy: 0.8530 - val_loss: 0.3331 - val_accuracy: 0.8439\n",
      "Epoch 20/100\n",
      "551/551 [==============================] - 0s 141us/sample - loss: 0.3488 - accuracy: 0.8512 - val_loss: 0.3270 - val_accuracy: 0.8481\n",
      "Epoch 21/100\n",
      "551/551 [==============================] - 0s 152us/sample - loss: 0.3426 - accuracy: 0.8494 - val_loss: 0.3209 - val_accuracy: 0.8523\n",
      "Epoch 22/100\n",
      "551/551 [==============================] - 0s 139us/sample - loss: 0.3372 - accuracy: 0.8494 - val_loss: 0.3162 - val_accuracy: 0.8523\n",
      "Epoch 23/100\n",
      "551/551 [==============================] - 0s 147us/sample - loss: 0.3321 - accuracy: 0.8548 - val_loss: 0.3118 - val_accuracy: 0.8565\n",
      "Epoch 24/100\n",
      "551/551 [==============================] - 0s 150us/sample - loss: 0.3278 - accuracy: 0.8548 - val_loss: 0.3078 - val_accuracy: 0.8565\n",
      "Epoch 25/100\n",
      "551/551 [==============================] - 0s 147us/sample - loss: 0.3236 - accuracy: 0.8566 - val_loss: 0.3042 - val_accuracy: 0.8565\n",
      "Epoch 26/100\n",
      "551/551 [==============================] - 0s 145us/sample - loss: 0.3196 - accuracy: 0.8639 - val_loss: 0.3012 - val_accuracy: 0.8565\n",
      "Epoch 27/100\n",
      "551/551 [==============================] - 0s 141us/sample - loss: 0.3159 - accuracy: 0.8621 - val_loss: 0.2986 - val_accuracy: 0.8565\n",
      "Epoch 28/100\n",
      "551/551 [==============================] - 0s 152us/sample - loss: 0.3125 - accuracy: 0.8657 - val_loss: 0.2953 - val_accuracy: 0.8565\n",
      "Epoch 29/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.3092 - accuracy: 0.8675 - val_loss: 0.2918 - val_accuracy: 0.8565\n",
      "Epoch 30/100\n",
      "551/551 [==============================] - 0s 146us/sample - loss: 0.3066 - accuracy: 0.8675 - val_loss: 0.2890 - val_accuracy: 0.8565\n",
      "Epoch 31/100\n",
      "551/551 [==============================] - 0s 146us/sample - loss: 0.3038 - accuracy: 0.8657 - val_loss: 0.2876 - val_accuracy: 0.8565\n",
      "Epoch 32/100\n",
      "551/551 [==============================] - 0s 140us/sample - loss: 0.3010 - accuracy: 0.8675 - val_loss: 0.2841 - val_accuracy: 0.8565\n",
      "Epoch 33/100\n",
      "551/551 [==============================] - 0s 168us/sample - loss: 0.2987 - accuracy: 0.8748 - val_loss: 0.2826 - val_accuracy: 0.8565\n",
      "Epoch 34/100\n",
      "551/551 [==============================] - 0s 147us/sample - loss: 0.2964 - accuracy: 0.8766 - val_loss: 0.2803 - val_accuracy: 0.8565\n",
      "Epoch 35/100\n",
      "551/551 [==============================] - 0s 145us/sample - loss: 0.2939 - accuracy: 0.8766 - val_loss: 0.2784 - val_accuracy: 0.8565\n",
      "Epoch 36/100\n",
      "551/551 [==============================] - 0s 158us/sample - loss: 0.2919 - accuracy: 0.8766 - val_loss: 0.2766 - val_accuracy: 0.8565\n",
      "Epoch 37/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.2900 - accuracy: 0.8766 - val_loss: 0.2741 - val_accuracy: 0.8565\n",
      "Epoch 38/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2883 - accuracy: 0.8766 - val_loss: 0.2730 - val_accuracy: 0.8565\n",
      "Epoch 39/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2868 - accuracy: 0.8784 - val_loss: 0.2709 - val_accuracy: 0.8565\n",
      "Epoch 40/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2850 - accuracy: 0.8802 - val_loss: 0.2702 - val_accuracy: 0.8565\n",
      "Epoch 41/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2835 - accuracy: 0.8802 - val_loss: 0.2687 - val_accuracy: 0.8608\n",
      "Epoch 42/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2819 - accuracy: 0.8820 - val_loss: 0.2677 - val_accuracy: 0.8608\n",
      "Epoch 43/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2807 - accuracy: 0.8820 - val_loss: 0.2666 - val_accuracy: 0.8608\n",
      "Epoch 44/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2792 - accuracy: 0.8838 - val_loss: 0.2659 - val_accuracy: 0.8650\n",
      "Epoch 45/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2781 - accuracy: 0.8802 - val_loss: 0.2655 - val_accuracy: 0.8650\n",
      "Epoch 46/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2770 - accuracy: 0.8838 - val_loss: 0.2644 - val_accuracy: 0.8650\n",
      "Epoch 47/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2761 - accuracy: 0.8838 - val_loss: 0.2634 - val_accuracy: 0.8650\n",
      "Epoch 48/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2749 - accuracy: 0.8838 - val_loss: 0.2630 - val_accuracy: 0.8650\n",
      "Epoch 49/100\n",
      "551/551 [==============================] - 0s 150us/sample - loss: 0.2738 - accuracy: 0.8838 - val_loss: 0.2620 - val_accuracy: 0.8650\n",
      "Epoch 50/100\n",
      "551/551 [==============================] - 0s 138us/sample - loss: 0.2731 - accuracy: 0.8857 - val_loss: 0.2612 - val_accuracy: 0.8650\n",
      "Epoch 51/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2722 - accuracy: 0.8857 - val_loss: 0.2608 - val_accuracy: 0.8650\n",
      "Epoch 52/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2712 - accuracy: 0.8838 - val_loss: 0.2597 - val_accuracy: 0.8650\n",
      "Epoch 53/100\n",
      "551/551 [==============================] - 0s 130us/sample - loss: 0.2705 - accuracy: 0.8857 - val_loss: 0.2583 - val_accuracy: 0.8650\n",
      "Epoch 54/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2694 - accuracy: 0.8857 - val_loss: 0.2584 - val_accuracy: 0.8692\n",
      "Epoch 55/100\n",
      "551/551 [==============================] - 0s 136us/sample - loss: 0.2687 - accuracy: 0.8857 - val_loss: 0.2584 - val_accuracy: 0.8692\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2677 - accuracy: 0.8838 - val_loss: 0.2575 - val_accuracy: 0.8692\n",
      "Epoch 57/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2667 - accuracy: 0.8857 - val_loss: 0.2569 - val_accuracy: 0.8692\n",
      "Epoch 58/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2660 - accuracy: 0.8857 - val_loss: 0.2571 - val_accuracy: 0.8692\n",
      "Epoch 59/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2654 - accuracy: 0.8838 - val_loss: 0.2570 - val_accuracy: 0.8692\n",
      "Epoch 60/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2645 - accuracy: 0.8820 - val_loss: 0.2559 - val_accuracy: 0.8692\n",
      "Epoch 61/100\n",
      "551/551 [==============================] - 0s 130us/sample - loss: 0.2637 - accuracy: 0.8875 - val_loss: 0.2553 - val_accuracy: 0.8692\n",
      "Epoch 62/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2631 - accuracy: 0.8857 - val_loss: 0.2548 - val_accuracy: 0.8692\n",
      "Epoch 63/100\n",
      "551/551 [==============================] - 0s 153us/sample - loss: 0.2623 - accuracy: 0.8857 - val_loss: 0.2551 - val_accuracy: 0.8692\n",
      "Epoch 64/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2615 - accuracy: 0.8875 - val_loss: 0.2542 - val_accuracy: 0.8734\n",
      "Epoch 65/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2607 - accuracy: 0.8875 - val_loss: 0.2534 - val_accuracy: 0.8734\n",
      "Epoch 66/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2598 - accuracy: 0.8893 - val_loss: 0.2529 - val_accuracy: 0.8692\n",
      "Epoch 67/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2591 - accuracy: 0.8893 - val_loss: 0.2519 - val_accuracy: 0.8692\n",
      "Epoch 68/100\n",
      "551/551 [==============================] - 0s 136us/sample - loss: 0.2581 - accuracy: 0.8893 - val_loss: 0.2515 - val_accuracy: 0.8692\n",
      "Epoch 69/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2574 - accuracy: 0.8893 - val_loss: 0.2517 - val_accuracy: 0.8692\n",
      "Epoch 70/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2564 - accuracy: 0.8893 - val_loss: 0.2508 - val_accuracy: 0.8692\n",
      "Epoch 71/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2560 - accuracy: 0.8911 - val_loss: 0.2500 - val_accuracy: 0.8692\n",
      "Epoch 72/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2554 - accuracy: 0.8893 - val_loss: 0.2510 - val_accuracy: 0.8776\n",
      "Epoch 73/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2550 - accuracy: 0.8893 - val_loss: 0.2510 - val_accuracy: 0.8776\n",
      "Epoch 74/100\n",
      "551/551 [==============================] - 0s 129us/sample - loss: 0.2545 - accuracy: 0.8893 - val_loss: 0.2511 - val_accuracy: 0.8776\n",
      "Epoch 75/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2532 - accuracy: 0.8893 - val_loss: 0.2498 - val_accuracy: 0.8776\n",
      "Epoch 76/100\n",
      "551/551 [==============================] - 0s 136us/sample - loss: 0.2525 - accuracy: 0.8893 - val_loss: 0.2492 - val_accuracy: 0.8776\n",
      "Epoch 77/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2520 - accuracy: 0.8893 - val_loss: 0.2494 - val_accuracy: 0.8776\n",
      "Epoch 78/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2512 - accuracy: 0.8911 - val_loss: 0.2490 - val_accuracy: 0.8776\n",
      "Epoch 79/100\n",
      "551/551 [==============================] - 0s 137us/sample - loss: 0.2506 - accuracy: 0.8929 - val_loss: 0.2493 - val_accuracy: 0.8776\n",
      "Epoch 80/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2498 - accuracy: 0.8911 - val_loss: 0.2488 - val_accuracy: 0.8776\n",
      "Epoch 81/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2493 - accuracy: 0.8911 - val_loss: 0.2487 - val_accuracy: 0.8776\n",
      "Epoch 82/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2487 - accuracy: 0.8911 - val_loss: 0.2486 - val_accuracy: 0.8776\n",
      "Epoch 83/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2481 - accuracy: 0.8911 - val_loss: 0.2484 - val_accuracy: 0.8776\n",
      "Epoch 84/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2473 - accuracy: 0.8947 - val_loss: 0.2494 - val_accuracy: 0.8776\n",
      "Epoch 85/100\n",
      "551/551 [==============================] - 0s 130us/sample - loss: 0.2470 - accuracy: 0.8947 - val_loss: 0.2500 - val_accuracy: 0.8776\n",
      "Epoch 86/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2464 - accuracy: 0.8947 - val_loss: 0.2491 - val_accuracy: 0.8776\n",
      "Epoch 87/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2461 - accuracy: 0.8947 - val_loss: 0.2497 - val_accuracy: 0.8776\n",
      "Epoch 88/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2451 - accuracy: 0.8947 - val_loss: 0.2497 - val_accuracy: 0.8776\n",
      "Epoch 89/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2448 - accuracy: 0.8966 - val_loss: 0.2479 - val_accuracy: 0.8819\n",
      "Epoch 90/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2442 - accuracy: 0.8984 - val_loss: 0.2474 - val_accuracy: 0.8819\n",
      "Epoch 91/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2436 - accuracy: 0.8947 - val_loss: 0.2477 - val_accuracy: 0.8819\n",
      "Epoch 92/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2432 - accuracy: 0.8947 - val_loss: 0.2480 - val_accuracy: 0.8819\n",
      "Epoch 93/100\n",
      "551/551 [==============================] - 0s 133us/sample - loss: 0.2424 - accuracy: 0.8966 - val_loss: 0.2477 - val_accuracy: 0.8819\n",
      "Epoch 94/100\n",
      "551/551 [==============================] - 0s 129us/sample - loss: 0.2419 - accuracy: 0.8966 - val_loss: 0.2473 - val_accuracy: 0.8819\n",
      "Epoch 95/100\n",
      "551/551 [==============================] - 0s 131us/sample - loss: 0.2414 - accuracy: 0.8966 - val_loss: 0.2472 - val_accuracy: 0.8819\n",
      "Epoch 96/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2411 - accuracy: 0.8966 - val_loss: 0.2467 - val_accuracy: 0.8819\n",
      "Epoch 97/100\n",
      "551/551 [==============================] - 0s 132us/sample - loss: 0.2405 - accuracy: 0.8984 - val_loss: 0.2465 - val_accuracy: 0.8819\n",
      "Epoch 98/100\n",
      "551/551 [==============================] - 0s 134us/sample - loss: 0.2400 - accuracy: 0.9002 - val_loss: 0.2459 - val_accuracy: 0.8819\n",
      "Epoch 99/100\n",
      "551/551 [==============================] - 0s 129us/sample - loss: 0.2393 - accuracy: 0.9002 - val_loss: 0.2451 - val_accuracy: 0.8819\n",
      "Epoch 100/100\n",
      "551/551 [==============================] - 0s 135us/sample - loss: 0.2392 - accuracy: 0.9002 - val_loss: 0.2443 - val_accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "model_simple = tf.keras.models.Sequential(name = 'ANN')\n",
    "\n",
    "model_simple.add(tf.keras.layers.Dense(6, activation='relu', input_shape=(9,), name='hidden1'))\n",
    "model_simple.add(tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model_simple.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model_simple.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1 - 0s - loss: 0.0920 - accuracy: 0.9400\n",
      "Test accuracy=0.9399999976158142\n"
     ]
    }
   ],
   "source": [
    "test_loss1, test_accuracy1 = model_simple.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy={test_accuracy1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done extensive model selection in the sections above. Nevetheless, we have identified a flaw with the above modeling approach. The flaw arises from the construction of the cross-validation set. Since for every year the data set contains two observations (one for the democrat candidate, one for the republican) if once of the two observations is in the training and the oter in the CV sets respectively than the model will easily recognize and memorize the pattern and therefore the model becomes near deterministic, i.e. there is a large information leakage. \n",
    "\n",
    "In order to prevent this issue, the first approach we will try is to get rid of half the observations, in other words we will only consider the observations for one (the republican candidate) for each election year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-do everything with the reduced dataframe in order to prevent information leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])\n",
    "df_republican = df[df.republican != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_republican[df_republican.Year != 2020]\n",
    "test_df = df_republican[df_republican.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Result', axis=1)#.values\n",
    "y_train = train_df['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(50, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.shape)\n",
    "display(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(X_train, y_train, models, scaler=True, Poly=1, PCA_comp=0):\n",
    "    # pca is the total variance explained required. 0 means that we do not want to perform pca\n",
    "    if 'State' in X_train.columns:\n",
    "        X_train = X_train.drop('State', axis=1)\n",
    "    if 'Year' in X_train.columns:\n",
    "        X_train = X_train.drop('Year', axis=1)\n",
    "    if PCA_comp > 0 and not scaler:\n",
    "        raise ValueError('We must normalize before performing PCA')\n",
    "    if scaler:\n",
    "        # print('Normalizing the data ...')\n",
    "        scaler = MinMaxScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "    if PCA_comp > 0:\n",
    "        # print('Performing PCA ...')\n",
    "        pca = PCA(n_components=X_train.shape[1])\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        total_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        for i, variance in enumerate(total_variance):\n",
    "            if variance > PCA_comp:\n",
    "                break \n",
    "        X_train = X_train[:, :i+1]\n",
    "    if Poly > 1:\n",
    "        # print('Polynomial transformation ...')\n",
    "        poly_features = PolynomialFeatures(degree=Poly, include_bias=False).fit(X_train)\n",
    "        X_train = poly_features.transform(X_train)\n",
    "    model_cv_accuracies = []\n",
    "    # print('Modelling and gathering the predictions ..')\n",
    "    for i, model in enumerate(models):\n",
    "        model_cv = cross_validate(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        model_cv_accuracy = np.mean(model_cv['test_score'])\n",
    "        model_cv_accuracies.append(model_cv_accuracy)\n",
    "    return models, model_cv_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8858487504057123]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=2000, penalty='l1', solver='saga')]\n",
    "models_11, cv_accuracies_11 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "print(cv_accuracies_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8808179162609543]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_12, cv_accuracies_12 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "print(cv_accuracies_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8808179162609543\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=2, include_bias=False).fit(X_train)\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train1 = scaler.transform(X_train)\n",
    "X_test1 = scaler.transform(X_test)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train1)\n",
    "X_test_poly = poly_features.transform(X_test1)\n",
    "model = LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')\n",
    "print(np.mean(cross_validate(model, X_train_poly, y_train, cv=5, scoring='accuracy')['test_score']))\n",
    "model.fit(X_train_poly, y_train)\n",
    "print(model.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8808179162609543]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_13, cv_accuracies_13 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "print(cv_accuracies_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8884453099642974]\n"
     ]
    }
   ],
   "source": [
    "models_considered = [LogisticRegressionCV(max_iter=5000, penalty='l1', solver='saga')]\n",
    "models_14, cv_accuracies_14 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "print(cv_accuracies_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree Classifier\n",
    "- PolynomialFeatures = 1, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0\n",
    "- PolynomialFeatures = 2, PCA = 0.9\n",
    "- PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.840181759169101\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8173644920480363\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8426160337552743\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8630314832846478\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8477117818889971\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8554365465757872\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8071080817916261\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8096397273612463\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8348912690684843\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_21, cv_accuracies_21 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_21[0], cv_accuracies_21[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8807530022719897\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8503083414475819\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8527426160337553\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.858033106134372\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8350535540408958\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8401493021746186\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8478091528724441\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8299253489126908\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.850308341447582\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_22, cv_accuracies_22 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0)\n",
    "    print(\"{0}: {1}\".format(models_22[0], cv_accuracies_22[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 2, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8757221681272315\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8731905225576112\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.8681272314183707\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8603700097370982\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.8604998377150276\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8503083414475819\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8299578059071729\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8273937033430704\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.8375202856215515\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_23, cv_accuracies_23 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=2, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_23[0], cv_accuracies_23[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolynomialFeatures = 1, PCA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=1): 0.8757221681272315\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=2): 0.8782538136968517\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=3): 0.86053229470951\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=4): 0.8451152223304123\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=5): 0.840084388185654\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=6): 0.8578059071729957\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=7): 0.8527750730282376\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=8): 0.8374878286270692\n",
      "-------------------------------------------------------\n",
      "DecisionTreeClassifier(max_depth=9): 0.842583576760792\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "depths = np.arange(1, 10, 1)\n",
    "for depth in depths:\n",
    "    models_considered = [DecisionTreeClassifier(max_depth=depth)]\n",
    "    models_24, cv_accuracies_24 = modelling(X_train, y_train, models=models_considered, scaler=True, Poly=1, PCA_comp=0.9)\n",
    "    print(\"{0}: {1}\".format(models_24[0], cv_accuracies_24[0]))\n",
    "    print('-'*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1728 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators_over = [50, 90, 110, 150, 200, 300, 400, 500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth_over = [int(x) for x in np.linspace(1, 10, num=6)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 3]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators_over,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth_over,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "\n",
    "               }\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "random_model = GridSearchCV(estimator=rf, param_grid=random_grid, verbose=1,\n",
    "                                    scoring='accuracy',\n",
    "                                    n_jobs=-1, return_train_score=True, cv=2, refit=True)\n",
    "random_model.fit(X_train, y_train)\n",
    "dict_rf = cross_validate(RandomForestClassifier(**random_model.best_params_), X_train, y_train, cv=9, scoring='accuracy')\n",
    "rf_initial_accuracy = np.mean(dict_rf['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 4, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 50}\n",
      "0.8910030537937514\n"
     ]
    }
   ],
   "source": [
    "print(random_model.best_params_)\n",
    "print(rf_initial_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(bootstrap = True, max_depth = 4, max_features = 'auto', min_samples_leaf = 1, min_samples_split = 3, n_estimators = 50)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  1.0 the CV score is  0.8784355179704018\n",
      "{'learning_rate': 0.1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  2.0 the CV score is  0.8554733380314775\n",
      "{'learning_rate': 0.01, 'n_estimators': 91}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  3.0 the CV score is  0.8759102654451492\n",
      "{'learning_rate': 1, 'n_estimators': 23}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  4.0 the CV score is  0.8759102654451492\n",
      "{'learning_rate': 1, 'n_estimators': 23}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  5.0 the CV score is  0.8577636833450787\n",
      "{'learning_rate': 1, 'n_estimators': 91}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 150 | elapsed:    1.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  6.0 the CV score is  0.8501291989664082\n",
      "{'learning_rate': 1, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  7.0 the CV score is  0.8759102654451492\n",
      "{'learning_rate': 0.1, 'n_estimators': 50}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  8.0 the CV score is  0.8554733380314775\n",
      "{'learning_rate': 0.01, 'n_estimators': 77}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  9.0 the CV score is  0.8784355179704016\n",
      "{'learning_rate': 0.1, 'n_estimators': 37}\n",
      "Fitting 2 folds for each of 75 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With components  10.0 the CV score is  0.8531242659149636\n",
      "{'learning_rate': 1, 'n_estimators': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Base Estimator\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 15)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "scores = []\n",
    "params_scaled = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = depth))\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 2, scoring = 'accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    dict_boosting = cross_validate(AdaBoostClassifier(**grid_search.best_params_), X_train, y_train, cv=9, scoring='accuracy', n_jobs=-1)\n",
    "    boosting_accuracy = np.mean(dict_boosting['test_score'])\n",
    "    print('With components ', depth, 'the CV score is ', boosting_accuracy)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 1), learning_rate = 0.1, n_estimators = 77).fit(X_train, y_train)\n",
    "ada.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('states/'+file) for file in os.listdir('states/')])\n",
    "df_republican = df[df.republican != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_republican[df_republican.Year != 2020]\n",
    "test_df = df_republican[df_republican.Year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Year', 'State'], axis = 1)\n",
    "test_df = test_df.drop(['Year', 'State'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Result', axis=1)#.values\n",
    "y_train = train_df['Result']#.values\n",
    "X_test = test_df.drop('Result', axis=1)#.values\n",
    "y_test = test_df['Result']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.797513</td>\n",
       "      <td>51.909088</td>\n",
       "      <td>51.657848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>53.37</td>\n",
       "      <td>106.259013</td>\n",
       "      <td>93086.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.651345</td>\n",
       "      <td>35.969860</td>\n",
       "      <td>38.271617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>37.45</td>\n",
       "      <td>113.717367</td>\n",
       "      <td>100758.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40.574781</td>\n",
       "      <td>41.630290</td>\n",
       "      <td>40.888825</td>\n",
       "      <td>1</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.5</td>\n",
       "      <td>40.72</td>\n",
       "      <td>123.352989</td>\n",
       "      <td>108448.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.459137</td>\n",
       "      <td>50.952625</td>\n",
       "      <td>44.657240</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.7</td>\n",
       "      <td>47.87</td>\n",
       "      <td>138.411248</td>\n",
       "      <td>115406.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54.915037</td>\n",
       "      <td>54.194542</td>\n",
       "      <td>55.351526</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50.73</td>\n",
       "      <td>147.528675</td>\n",
       "      <td>123452.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_10   month_11    month_9  republican  Rep_House_Prop  rep_loyalty  \\\n",
       "0  51.797513  51.909088  51.657848           1        0.200000          0.5   \n",
       "3  35.651345  35.969860  38.271617           1        0.100000          0.7   \n",
       "5  40.574781  41.630290  40.888825           1        0.363636          0.5   \n",
       "7  48.459137  50.952625  44.657240           1        0.727273          0.7   \n",
       "8  54.915037  54.194542  55.351526           1        0.727273          0.8   \n",
       "\n",
       "   popular_vote_percentage     density        RDI  \n",
       "0                    53.37  106.259013   93086.74  \n",
       "3                    37.45  113.717367  100758.19  \n",
       "5                    40.72  123.352989  108448.50  \n",
       "7                    47.87  138.411248  115406.50  \n",
       "8                    50.73  147.528675  123452.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_9</th>\n",
       "      <th>republican</th>\n",
       "      <th>Rep_House_Prop</th>\n",
       "      <th>rep_loyalty</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>density</th>\n",
       "      <th>RDI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.657139</td>\n",
       "      <td>47.298710</td>\n",
       "      <td>47.170450</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>180.617755</td>\n",
       "      <td>163524.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46.311941</td>\n",
       "      <td>47.027987</td>\n",
       "      <td>46.593060</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.8</td>\n",
       "      <td>46.525</td>\n",
       "      <td>197.170250</td>\n",
       "      <td>161835.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66.209900</td>\n",
       "      <td>62.336910</td>\n",
       "      <td>66.226278</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>5.796735</td>\n",
       "      <td>10593.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>42.653251</td>\n",
       "      <td>42.836953</td>\n",
       "      <td>43.010549</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.525</td>\n",
       "      <td>146.642070</td>\n",
       "      <td>29220.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>43.585778</td>\n",
       "      <td>43.795763</td>\n",
       "      <td>43.493822</td>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>46.525</td>\n",
       "      <td>89.335603</td>\n",
       "      <td>119627.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month_10   month_11    month_9  republican  Rep_House_Prop  rep_loyalty  \\\n",
       "16  46.657139  47.298710  47.170450           1        0.714286          1.0   \n",
       "16  46.311941  47.027987  46.593060           1        0.785714          0.8   \n",
       "12  66.209900  62.336910  66.226278           1        1.000000          1.0   \n",
       "16  42.653251  42.836953  43.010549           1        0.000000          0.0   \n",
       "16  43.585778  43.795763  43.493822           1        0.625000          0.4   \n",
       "\n",
       "    popular_vote_percentage     density        RDI  \n",
       "16                   46.525  180.617755  163524.50  \n",
       "16                   46.525  197.170250  161835.04  \n",
       "12                   46.525    5.796735   10593.19  \n",
       "16                   46.525  146.642070   29220.57  \n",
       "16                   46.525   89.335603  119627.08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Defining an Early Stopping Condition\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 50)\n",
    "regularizer = 0.00015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315 samples, validate on 79 samples\n",
      "Epoch 1/40\n",
      "315/315 - 1s - loss: 0.7072 - accuracy: 0.5778 - val_loss: 0.5951 - val_accuracy: 0.8101\n",
      "Epoch 2/40\n",
      "315/315 - 0s - loss: 0.6874 - accuracy: 0.5810 - val_loss: 0.5897 - val_accuracy: 0.8101\n",
      "Epoch 3/40\n",
      "315/315 - 0s - loss: 0.7118 - accuracy: 0.6063 - val_loss: 0.5820 - val_accuracy: 0.8101\n",
      "Epoch 4/40\n",
      "315/315 - 0s - loss: 0.6703 - accuracy: 0.6317 - val_loss: 0.5707 - val_accuracy: 0.8101\n",
      "Epoch 5/40\n",
      "315/315 - 0s - loss: 0.6750 - accuracy: 0.6222 - val_loss: 0.5565 - val_accuracy: 0.8101\n",
      "Epoch 6/40\n",
      "315/315 - 0s - loss: 0.6860 - accuracy: 0.6508 - val_loss: 0.5485 - val_accuracy: 0.8101\n",
      "Epoch 7/40\n",
      "315/315 - 0s - loss: 0.6767 - accuracy: 0.6222 - val_loss: 0.5374 - val_accuracy: 0.8101\n",
      "Epoch 8/40\n",
      "315/315 - 0s - loss: 0.6365 - accuracy: 0.6667 - val_loss: 0.5244 - val_accuracy: 0.8228\n",
      "Epoch 9/40\n",
      "315/315 - 0s - loss: 0.6683 - accuracy: 0.6444 - val_loss: 0.5141 - val_accuracy: 0.8228\n",
      "Epoch 10/40\n",
      "315/315 - 0s - loss: 0.6442 - accuracy: 0.6857 - val_loss: 0.5081 - val_accuracy: 0.7975\n",
      "Epoch 11/40\n",
      "315/315 - 0s - loss: 0.6558 - accuracy: 0.6413 - val_loss: 0.4963 - val_accuracy: 0.8354\n",
      "Epoch 12/40\n",
      "315/315 - 0s - loss: 0.6199 - accuracy: 0.6984 - val_loss: 0.4857 - val_accuracy: 0.8481\n",
      "Epoch 13/40\n",
      "315/315 - 0s - loss: 0.6097 - accuracy: 0.6825 - val_loss: 0.4695 - val_accuracy: 0.8481\n",
      "Epoch 14/40\n",
      "315/315 - 0s - loss: 0.6043 - accuracy: 0.6825 - val_loss: 0.4561 - val_accuracy: 0.8481\n",
      "Epoch 15/40\n",
      "315/315 - 0s - loss: 0.6048 - accuracy: 0.6762 - val_loss: 0.4435 - val_accuracy: 0.8734\n",
      "Epoch 16/40\n",
      "315/315 - 0s - loss: 0.5947 - accuracy: 0.7048 - val_loss: 0.4336 - val_accuracy: 0.8734\n",
      "Epoch 17/40\n",
      "315/315 - 0s - loss: 0.5854 - accuracy: 0.7143 - val_loss: 0.4199 - val_accuracy: 0.8734\n",
      "Epoch 18/40\n",
      "315/315 - 0s - loss: 0.5517 - accuracy: 0.7492 - val_loss: 0.4075 - val_accuracy: 0.8734\n",
      "Epoch 19/40\n",
      "315/315 - 0s - loss: 0.5400 - accuracy: 0.7556 - val_loss: 0.3926 - val_accuracy: 0.8734\n",
      "Epoch 20/40\n",
      "315/315 - 0s - loss: 0.5441 - accuracy: 0.7683 - val_loss: 0.3834 - val_accuracy: 0.8734\n",
      "Epoch 21/40\n",
      "315/315 - 0s - loss: 0.5380 - accuracy: 0.7429 - val_loss: 0.3691 - val_accuracy: 0.8734\n",
      "Epoch 22/40\n",
      "315/315 - 0s - loss: 0.5099 - accuracy: 0.7619 - val_loss: 0.3579 - val_accuracy: 0.8734\n",
      "Epoch 23/40\n",
      "315/315 - 0s - loss: 0.5365 - accuracy: 0.7524 - val_loss: 0.3471 - val_accuracy: 0.8861\n",
      "Epoch 24/40\n",
      "315/315 - 0s - loss: 0.5151 - accuracy: 0.7556 - val_loss: 0.3361 - val_accuracy: 0.8987\n",
      "Epoch 25/40\n",
      "315/315 - 0s - loss: 0.5181 - accuracy: 0.7651 - val_loss: 0.3275 - val_accuracy: 0.8987\n",
      "Epoch 26/40\n",
      "315/315 - 0s - loss: 0.5106 - accuracy: 0.7714 - val_loss: 0.3211 - val_accuracy: 0.8987\n",
      "Epoch 27/40\n",
      "315/315 - 0s - loss: 0.5011 - accuracy: 0.8032 - val_loss: 0.3154 - val_accuracy: 0.8987\n",
      "Epoch 28/40\n",
      "315/315 - 0s - loss: 0.4667 - accuracy: 0.8127 - val_loss: 0.3086 - val_accuracy: 0.8987\n",
      "Epoch 29/40\n",
      "315/315 - 0s - loss: 0.4674 - accuracy: 0.7968 - val_loss: 0.3020 - val_accuracy: 0.8987\n",
      "Epoch 30/40\n",
      "315/315 - 0s - loss: 0.4722 - accuracy: 0.7651 - val_loss: 0.2961 - val_accuracy: 0.8861\n",
      "Epoch 31/40\n",
      "315/315 - 0s - loss: 0.4852 - accuracy: 0.7619 - val_loss: 0.2909 - val_accuracy: 0.8861\n",
      "Epoch 32/40\n",
      "315/315 - 0s - loss: 0.4487 - accuracy: 0.8095 - val_loss: 0.2834 - val_accuracy: 0.8987\n",
      "Epoch 33/40\n",
      "315/315 - 0s - loss: 0.4637 - accuracy: 0.7873 - val_loss: 0.2757 - val_accuracy: 0.8861\n",
      "Epoch 34/40\n",
      "315/315 - 0s - loss: 0.4614 - accuracy: 0.8000 - val_loss: 0.2741 - val_accuracy: 0.8987\n",
      "Epoch 35/40\n",
      "315/315 - 0s - loss: 0.4554 - accuracy: 0.7746 - val_loss: 0.2727 - val_accuracy: 0.8987\n",
      "Epoch 36/40\n",
      "315/315 - 0s - loss: 0.4415 - accuracy: 0.8063 - val_loss: 0.2694 - val_accuracy: 0.8987\n",
      "Epoch 37/40\n",
      "315/315 - 0s - loss: 0.4826 - accuracy: 0.7968 - val_loss: 0.2699 - val_accuracy: 0.9114\n",
      "Epoch 38/40\n",
      "315/315 - 0s - loss: 0.4402 - accuracy: 0.7968 - val_loss: 0.2671 - val_accuracy: 0.9114\n",
      "Epoch 39/40\n",
      "315/315 - 0s - loss: 0.4472 - accuracy: 0.8095 - val_loss: 0.2639 - val_accuracy: 0.8987\n",
      "Epoch 40/40\n",
      "315/315 - 0s - loss: 0.4330 - accuracy: 0.8222 - val_loss: 0.2632 - val_accuracy: 0.8987\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(name = 'ANN')\n",
    "\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu', input_shape=(9,), name='hidden1', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu', name='hidden2', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(4, activation='relu', name='hidden3', kernel_regularizer = tf.keras.regularizers.l1(regularizer)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=40, batch_size=16, verbose=2, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/1 - 0s - loss: 0.1938 - accuracy: 0.9200\n",
      "Test accuracy=0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy={test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 275 samples, validate on 119 samples\n",
      "Epoch 1/150\n",
      "275/275 [==============================] - 1s 3ms/sample - loss: 0.6253 - accuracy: 0.6036 - val_loss: 0.5266 - val_accuracy: 0.6891\n",
      "Epoch 2/150\n",
      "275/275 [==============================] - 0s 240us/sample - loss: 0.6074 - accuracy: 0.6291 - val_loss: 0.5124 - val_accuracy: 0.7227\n",
      "Epoch 3/150\n",
      "275/275 [==============================] - 0s 273us/sample - loss: 0.5915 - accuracy: 0.6945 - val_loss: 0.4990 - val_accuracy: 0.7647\n",
      "Epoch 4/150\n",
      "275/275 [==============================] - 0s 307us/sample - loss: 0.5763 - accuracy: 0.7309 - val_loss: 0.4871 - val_accuracy: 0.7983\n",
      "Epoch 5/150\n",
      "275/275 [==============================] - 0s 306us/sample - loss: 0.5625 - accuracy: 0.7636 - val_loss: 0.4757 - val_accuracy: 0.8151\n",
      "Epoch 6/150\n",
      "275/275 [==============================] - 0s 258us/sample - loss: 0.5487 - accuracy: 0.7782 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
      "Epoch 7/150\n",
      "275/275 [==============================] - 0s 252us/sample - loss: 0.5367 - accuracy: 0.7927 - val_loss: 0.4549 - val_accuracy: 0.8151\n",
      "Epoch 8/150\n",
      "275/275 [==============================] - 0s 252us/sample - loss: 0.5242 - accuracy: 0.8145 - val_loss: 0.4455 - val_accuracy: 0.8151\n",
      "Epoch 9/150\n",
      "275/275 [==============================] - 0s 270us/sample - loss: 0.5133 - accuracy: 0.8291 - val_loss: 0.4364 - val_accuracy: 0.8151\n",
      "Epoch 10/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.5023 - accuracy: 0.8327 - val_loss: 0.4279 - val_accuracy: 0.8151\n",
      "Epoch 11/150\n",
      "275/275 [==============================] - 0s 262us/sample - loss: 0.4920 - accuracy: 0.8400 - val_loss: 0.4199 - val_accuracy: 0.8319\n",
      "Epoch 12/150\n",
      "275/275 [==============================] - 0s 325us/sample - loss: 0.4823 - accuracy: 0.8436 - val_loss: 0.4120 - val_accuracy: 0.8319\n",
      "Epoch 13/150\n",
      "275/275 [==============================] - 0s 251us/sample - loss: 0.4728 - accuracy: 0.8473 - val_loss: 0.4043 - val_accuracy: 0.8403\n",
      "Epoch 14/150\n",
      "275/275 [==============================] - 0s 260us/sample - loss: 0.4635 - accuracy: 0.8436 - val_loss: 0.3971 - val_accuracy: 0.8403\n",
      "Epoch 15/150\n",
      "275/275 [==============================] - 0s 252us/sample - loss: 0.4548 - accuracy: 0.8509 - val_loss: 0.3900 - val_accuracy: 0.8403\n",
      "Epoch 16/150\n",
      "275/275 [==============================] - 0s 240us/sample - loss: 0.4464 - accuracy: 0.8545 - val_loss: 0.3833 - val_accuracy: 0.8403\n",
      "Epoch 17/150\n",
      "275/275 [==============================] - 0s 252us/sample - loss: 0.4382 - accuracy: 0.8545 - val_loss: 0.3768 - val_accuracy: 0.8403\n",
      "Epoch 18/150\n",
      "275/275 [==============================] - 0s 242us/sample - loss: 0.4306 - accuracy: 0.8691 - val_loss: 0.3705 - val_accuracy: 0.8487\n",
      "Epoch 19/150\n",
      "275/275 [==============================] - 0s 235us/sample - loss: 0.4233 - accuracy: 0.8727 - val_loss: 0.3644 - val_accuracy: 0.8571\n",
      "Epoch 20/150\n",
      "275/275 [==============================] - 0s 253us/sample - loss: 0.4161 - accuracy: 0.8727 - val_loss: 0.3589 - val_accuracy: 0.8571\n",
      "Epoch 21/150\n",
      "275/275 [==============================] - 0s 229us/sample - loss: 0.4097 - accuracy: 0.8800 - val_loss: 0.3535 - val_accuracy: 0.8571\n",
      "Epoch 22/150\n",
      "275/275 [==============================] - 0s 233us/sample - loss: 0.4033 - accuracy: 0.8836 - val_loss: 0.3485 - val_accuracy: 0.8571\n",
      "Epoch 23/150\n",
      "275/275 [==============================] - 0s 255us/sample - loss: 0.3972 - accuracy: 0.8836 - val_loss: 0.3432 - val_accuracy: 0.8571\n",
      "Epoch 24/150\n",
      "275/275 [==============================] - 0s 237us/sample - loss: 0.3912 - accuracy: 0.8836 - val_loss: 0.3384 - val_accuracy: 0.8487\n",
      "Epoch 25/150\n",
      "275/275 [==============================] - 0s 251us/sample - loss: 0.3855 - accuracy: 0.8836 - val_loss: 0.3338 - val_accuracy: 0.8487\n",
      "Epoch 26/150\n",
      "275/275 [==============================] - 0s 258us/sample - loss: 0.3801 - accuracy: 0.8800 - val_loss: 0.3294 - val_accuracy: 0.8487\n",
      "Epoch 27/150\n",
      "275/275 [==============================] - 0s 261us/sample - loss: 0.3750 - accuracy: 0.8800 - val_loss: 0.3253 - val_accuracy: 0.8487\n",
      "Epoch 28/150\n",
      "275/275 [==============================] - 0s 273us/sample - loss: 0.3699 - accuracy: 0.8764 - val_loss: 0.3212 - val_accuracy: 0.8487\n",
      "Epoch 29/150\n",
      "275/275 [==============================] - 0s 276us/sample - loss: 0.3651 - accuracy: 0.8727 - val_loss: 0.3174 - val_accuracy: 0.8487\n",
      "Epoch 30/150\n",
      "275/275 [==============================] - 0s 258us/sample - loss: 0.3606 - accuracy: 0.8764 - val_loss: 0.3137 - val_accuracy: 0.8487\n",
      "Epoch 31/150\n",
      "275/275 [==============================] - 0s 235us/sample - loss: 0.3562 - accuracy: 0.8800 - val_loss: 0.3105 - val_accuracy: 0.8487\n",
      "Epoch 32/150\n",
      "275/275 [==============================] - 0s 258us/sample - loss: 0.3521 - accuracy: 0.8836 - val_loss: 0.3070 - val_accuracy: 0.8487\n",
      "Epoch 33/150\n",
      "275/275 [==============================] - 0s 256us/sample - loss: 0.3479 - accuracy: 0.8836 - val_loss: 0.3038 - val_accuracy: 0.8487\n",
      "Epoch 34/150\n",
      "275/275 [==============================] - 0s 255us/sample - loss: 0.3442 - accuracy: 0.8800 - val_loss: 0.3007 - val_accuracy: 0.8571\n",
      "Epoch 35/150\n",
      "275/275 [==============================] - 0s 221us/sample - loss: 0.3405 - accuracy: 0.8836 - val_loss: 0.2977 - val_accuracy: 0.8571\n",
      "Epoch 36/150\n",
      "275/275 [==============================] - 0s 213us/sample - loss: 0.3369 - accuracy: 0.8836 - val_loss: 0.2949 - val_accuracy: 0.8571\n",
      "Epoch 37/150\n",
      "275/275 [==============================] - 0s 258us/sample - loss: 0.3337 - accuracy: 0.8836 - val_loss: 0.2922 - val_accuracy: 0.8571\n",
      "Epoch 38/150\n",
      "275/275 [==============================] - 0s 289us/sample - loss: 0.3304 - accuracy: 0.8836 - val_loss: 0.2899 - val_accuracy: 0.8571\n",
      "Epoch 39/150\n",
      "275/275 [==============================] - 0s 271us/sample - loss: 0.3274 - accuracy: 0.8800 - val_loss: 0.2877 - val_accuracy: 0.8571\n",
      "Epoch 40/150\n",
      "275/275 [==============================] - 0s 257us/sample - loss: 0.3244 - accuracy: 0.8800 - val_loss: 0.2857 - val_accuracy: 0.8655\n",
      "Epoch 41/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.3216 - accuracy: 0.8764 - val_loss: 0.2836 - val_accuracy: 0.8655\n",
      "Epoch 42/150\n",
      "275/275 [==============================] - 0s 253us/sample - loss: 0.3188 - accuracy: 0.8764 - val_loss: 0.2816 - val_accuracy: 0.8655\n",
      "Epoch 43/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.3163 - accuracy: 0.8764 - val_loss: 0.2797 - val_accuracy: 0.8655\n",
      "Epoch 44/150\n",
      "275/275 [==============================] - 0s 227us/sample - loss: 0.3138 - accuracy: 0.8764 - val_loss: 0.2779 - val_accuracy: 0.8824\n",
      "Epoch 45/150\n",
      "275/275 [==============================] - 0s 231us/sample - loss: 0.3114 - accuracy: 0.8800 - val_loss: 0.2758 - val_accuracy: 0.8824\n",
      "Epoch 46/150\n",
      "275/275 [==============================] - 0s 234us/sample - loss: 0.3089 - accuracy: 0.8800 - val_loss: 0.2744 - val_accuracy: 0.8739\n",
      "Epoch 47/150\n",
      "275/275 [==============================] - 0s 230us/sample - loss: 0.3069 - accuracy: 0.8800 - val_loss: 0.2728 - val_accuracy: 0.8739\n",
      "Epoch 48/150\n",
      "275/275 [==============================] - 0s 271us/sample - loss: 0.3049 - accuracy: 0.8800 - val_loss: 0.2710 - val_accuracy: 0.8824\n",
      "Epoch 49/150\n",
      "275/275 [==============================] - 0s 266us/sample - loss: 0.3028 - accuracy: 0.8800 - val_loss: 0.2696 - val_accuracy: 0.8824\n",
      "Epoch 50/150\n",
      "275/275 [==============================] - 0s 215us/sample - loss: 0.3008 - accuracy: 0.8800 - val_loss: 0.2686 - val_accuracy: 0.8824\n",
      "Epoch 51/150\n",
      "275/275 [==============================] - 0s 254us/sample - loss: 0.2989 - accuracy: 0.8764 - val_loss: 0.2674 - val_accuracy: 0.8739\n",
      "Epoch 52/150\n",
      "275/275 [==============================] - 0s 320us/sample - loss: 0.2972 - accuracy: 0.8764 - val_loss: 0.2663 - val_accuracy: 0.8739\n",
      "Epoch 53/150\n",
      "275/275 [==============================] - 0s 269us/sample - loss: 0.2954 - accuracy: 0.8764 - val_loss: 0.2653 - val_accuracy: 0.8739\n",
      "Epoch 54/150\n",
      "275/275 [==============================] - 0s 272us/sample - loss: 0.2938 - accuracy: 0.8764 - val_loss: 0.2643 - val_accuracy: 0.8739\n",
      "Epoch 55/150\n",
      "275/275 [==============================] - 0s 214us/sample - loss: 0.2921 - accuracy: 0.8764 - val_loss: 0.2635 - val_accuracy: 0.8739\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/275 [==============================] - 0s 257us/sample - loss: 0.2906 - accuracy: 0.8764 - val_loss: 0.2628 - val_accuracy: 0.8739\n",
      "Epoch 57/150\n",
      "275/275 [==============================] - 0s 247us/sample - loss: 0.2892 - accuracy: 0.8764 - val_loss: 0.2621 - val_accuracy: 0.8739\n",
      "Epoch 58/150\n",
      "275/275 [==============================] - 0s 263us/sample - loss: 0.2876 - accuracy: 0.8764 - val_loss: 0.2612 - val_accuracy: 0.8739\n",
      "Epoch 59/150\n",
      "275/275 [==============================] - 0s 249us/sample - loss: 0.2862 - accuracy: 0.8764 - val_loss: 0.2604 - val_accuracy: 0.8739\n",
      "Epoch 60/150\n",
      "275/275 [==============================] - 0s 227us/sample - loss: 0.2848 - accuracy: 0.8764 - val_loss: 0.2598 - val_accuracy: 0.8739\n",
      "Epoch 61/150\n",
      "275/275 [==============================] - 0s 234us/sample - loss: 0.2835 - accuracy: 0.8764 - val_loss: 0.2592 - val_accuracy: 0.8739\n",
      "Epoch 62/150\n",
      "275/275 [==============================] - 0s 262us/sample - loss: 0.2823 - accuracy: 0.8764 - val_loss: 0.2587 - val_accuracy: 0.8739\n",
      "Epoch 63/150\n",
      "275/275 [==============================] - 0s 238us/sample - loss: 0.2811 - accuracy: 0.8764 - val_loss: 0.2581 - val_accuracy: 0.8739\n",
      "Epoch 64/150\n",
      "275/275 [==============================] - 0s 247us/sample - loss: 0.2797 - accuracy: 0.8764 - val_loss: 0.2577 - val_accuracy: 0.8739\n",
      "Epoch 65/150\n",
      "275/275 [==============================] - 0s 242us/sample - loss: 0.2788 - accuracy: 0.8764 - val_loss: 0.2569 - val_accuracy: 0.8739\n",
      "Epoch 66/150\n",
      "275/275 [==============================] - 0s 280us/sample - loss: 0.2776 - accuracy: 0.8764 - val_loss: 0.2566 - val_accuracy: 0.8739\n",
      "Epoch 67/150\n",
      "275/275 [==============================] - 0s 252us/sample - loss: 0.2765 - accuracy: 0.8800 - val_loss: 0.2563 - val_accuracy: 0.8739\n",
      "Epoch 68/150\n",
      "275/275 [==============================] - 0s 248us/sample - loss: 0.2755 - accuracy: 0.8800 - val_loss: 0.2559 - val_accuracy: 0.8739\n",
      "Epoch 69/150\n",
      "275/275 [==============================] - 0s 262us/sample - loss: 0.2745 - accuracy: 0.8800 - val_loss: 0.2557 - val_accuracy: 0.8739\n",
      "Epoch 70/150\n",
      "275/275 [==============================] - 0s 222us/sample - loss: 0.2735 - accuracy: 0.8800 - val_loss: 0.2552 - val_accuracy: 0.8739\n",
      "Epoch 71/150\n",
      "275/275 [==============================] - 0s 270us/sample - loss: 0.2725 - accuracy: 0.8800 - val_loss: 0.2550 - val_accuracy: 0.8739\n",
      "Epoch 72/150\n",
      "275/275 [==============================] - 0s 245us/sample - loss: 0.2716 - accuracy: 0.8800 - val_loss: 0.2548 - val_accuracy: 0.8739\n",
      "Epoch 73/150\n",
      "275/275 [==============================] - 0s 239us/sample - loss: 0.2708 - accuracy: 0.8800 - val_loss: 0.2545 - val_accuracy: 0.8739\n",
      "Epoch 74/150\n",
      "275/275 [==============================] - 0s 241us/sample - loss: 0.2701 - accuracy: 0.8800 - val_loss: 0.2539 - val_accuracy: 0.8739\n",
      "Epoch 75/150\n",
      "275/275 [==============================] - 0s 263us/sample - loss: 0.2690 - accuracy: 0.8800 - val_loss: 0.2540 - val_accuracy: 0.8739\n",
      "Epoch 76/150\n",
      "275/275 [==============================] - 0s 284us/sample - loss: 0.2682 - accuracy: 0.8800 - val_loss: 0.2539 - val_accuracy: 0.8739\n",
      "Epoch 77/150\n",
      "275/275 [==============================] - 0s 269us/sample - loss: 0.2674 - accuracy: 0.8800 - val_loss: 0.2539 - val_accuracy: 0.8739\n",
      "Epoch 78/150\n",
      "275/275 [==============================] - 0s 271us/sample - loss: 0.2666 - accuracy: 0.8800 - val_loss: 0.2536 - val_accuracy: 0.8824\n",
      "Epoch 79/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.2658 - accuracy: 0.8800 - val_loss: 0.2537 - val_accuracy: 0.8824\n",
      "Epoch 80/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.2651 - accuracy: 0.8800 - val_loss: 0.2537 - val_accuracy: 0.8824\n",
      "Epoch 81/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.2643 - accuracy: 0.8800 - val_loss: 0.2535 - val_accuracy: 0.8824\n",
      "Epoch 82/150\n",
      "275/275 [==============================] - 0s 231us/sample - loss: 0.2637 - accuracy: 0.8800 - val_loss: 0.2533 - val_accuracy: 0.8824\n",
      "Epoch 83/150\n",
      "275/275 [==============================] - 0s 229us/sample - loss: 0.2631 - accuracy: 0.8800 - val_loss: 0.2532 - val_accuracy: 0.8824\n",
      "Epoch 84/150\n",
      "275/275 [==============================] - 0s 260us/sample - loss: 0.2623 - accuracy: 0.8800 - val_loss: 0.2530 - val_accuracy: 0.8824\n",
      "Epoch 85/150\n",
      "275/275 [==============================] - 0s 259us/sample - loss: 0.2617 - accuracy: 0.8800 - val_loss: 0.2532 - val_accuracy: 0.8824\n",
      "Epoch 86/150\n",
      "275/275 [==============================] - 0s 231us/sample - loss: 0.2610 - accuracy: 0.8800 - val_loss: 0.2531 - val_accuracy: 0.8824\n",
      "Epoch 87/150\n",
      "275/275 [==============================] - 0s 233us/sample - loss: 0.2604 - accuracy: 0.8800 - val_loss: 0.2531 - val_accuracy: 0.8824\n",
      "Epoch 88/150\n",
      "275/275 [==============================] - 0s 211us/sample - loss: 0.2598 - accuracy: 0.8800 - val_loss: 0.2531 - val_accuracy: 0.8824\n",
      "Epoch 89/150\n",
      "275/275 [==============================] - 0s 234us/sample - loss: 0.2594 - accuracy: 0.8800 - val_loss: 0.2527 - val_accuracy: 0.8824\n",
      "Epoch 90/150\n",
      "275/275 [==============================] - 0s 233us/sample - loss: 0.2587 - accuracy: 0.8800 - val_loss: 0.2528 - val_accuracy: 0.8824\n",
      "Epoch 91/150\n",
      "275/275 [==============================] - 0s 238us/sample - loss: 0.2581 - accuracy: 0.8800 - val_loss: 0.2527 - val_accuracy: 0.8824\n",
      "Epoch 92/150\n",
      "275/275 [==============================] - 0s 273us/sample - loss: 0.2577 - accuracy: 0.8800 - val_loss: 0.2529 - val_accuracy: 0.8824\n",
      "Epoch 93/150\n",
      "275/275 [==============================] - 0s 270us/sample - loss: 0.2572 - accuracy: 0.8800 - val_loss: 0.2531 - val_accuracy: 0.8824\n",
      "Epoch 94/150\n",
      "275/275 [==============================] - 0s 251us/sample - loss: 0.2565 - accuracy: 0.8836 - val_loss: 0.2531 - val_accuracy: 0.8824\n",
      "Epoch 95/150\n",
      "275/275 [==============================] - 0s 245us/sample - loss: 0.2560 - accuracy: 0.8836 - val_loss: 0.2530 - val_accuracy: 0.8824\n",
      "Epoch 96/150\n",
      "275/275 [==============================] - 0s 218us/sample - loss: 0.2556 - accuracy: 0.8836 - val_loss: 0.2532 - val_accuracy: 0.8824\n",
      "Epoch 97/150\n",
      "275/275 [==============================] - 0s 232us/sample - loss: 0.2551 - accuracy: 0.8836 - val_loss: 0.2534 - val_accuracy: 0.8824\n",
      "Epoch 98/150\n",
      "275/275 [==============================] - 0s 261us/sample - loss: 0.2545 - accuracy: 0.8836 - val_loss: 0.2534 - val_accuracy: 0.8824\n",
      "Epoch 99/150\n",
      "275/275 [==============================] - 0s 225us/sample - loss: 0.2540 - accuracy: 0.8836 - val_loss: 0.2534 - val_accuracy: 0.8824\n",
      "Epoch 100/150\n",
      "275/275 [==============================] - 0s 276us/sample - loss: 0.2536 - accuracy: 0.8836 - val_loss: 0.2535 - val_accuracy: 0.8824\n",
      "Epoch 101/150\n",
      "275/275 [==============================] - 0s 248us/sample - loss: 0.2532 - accuracy: 0.8836 - val_loss: 0.2534 - val_accuracy: 0.8824\n",
      "Epoch 102/150\n",
      "275/275 [==============================] - 0s 217us/sample - loss: 0.2527 - accuracy: 0.8836 - val_loss: 0.2537 - val_accuracy: 0.8824\n",
      "Epoch 103/150\n",
      "275/275 [==============================] - 0s 282us/sample - loss: 0.2523 - accuracy: 0.8836 - val_loss: 0.2538 - val_accuracy: 0.8824\n",
      "Epoch 104/150\n",
      "275/275 [==============================] - 0s 266us/sample - loss: 0.2519 - accuracy: 0.8836 - val_loss: 0.2540 - val_accuracy: 0.8824\n",
      "Epoch 105/150\n",
      "275/275 [==============================] - 0s 266us/sample - loss: 0.2515 - accuracy: 0.8873 - val_loss: 0.2544 - val_accuracy: 0.8824\n",
      "Epoch 106/150\n",
      "275/275 [==============================] - 0s 238us/sample - loss: 0.2511 - accuracy: 0.8873 - val_loss: 0.2545 - val_accuracy: 0.8824\n",
      "Epoch 107/150\n",
      "275/275 [==============================] - 0s 249us/sample - loss: 0.2506 - accuracy: 0.8873 - val_loss: 0.2547 - val_accuracy: 0.8824\n",
      "Epoch 108/150\n",
      "275/275 [==============================] - 0s 239us/sample - loss: 0.2504 - accuracy: 0.8873 - val_loss: 0.2549 - val_accuracy: 0.8824\n",
      "Epoch 109/150\n",
      "275/275 [==============================] - 0s 251us/sample - loss: 0.2500 - accuracy: 0.8873 - val_loss: 0.2550 - val_accuracy: 0.8824\n",
      "Epoch 110/150\n",
      "275/275 [==============================] - 0s 222us/sample - loss: 0.2496 - accuracy: 0.8873 - val_loss: 0.2552 - val_accuracy: 0.8824\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/275 [==============================] - 0s 238us/sample - loss: 0.2493 - accuracy: 0.8873 - val_loss: 0.2555 - val_accuracy: 0.8824\n",
      "Epoch 112/150\n",
      "275/275 [==============================] - 0s 247us/sample - loss: 0.2489 - accuracy: 0.8909 - val_loss: 0.2557 - val_accuracy: 0.8824\n",
      "Epoch 113/150\n",
      "275/275 [==============================] - 0s 239us/sample - loss: 0.2486 - accuracy: 0.8909 - val_loss: 0.2557 - val_accuracy: 0.8824\n",
      "Epoch 114/150\n",
      "275/275 [==============================] - 0s 220us/sample - loss: 0.2484 - accuracy: 0.8873 - val_loss: 0.2559 - val_accuracy: 0.8824\n",
      "Epoch 115/150\n",
      "275/275 [==============================] - 0s 191us/sample - loss: 0.2479 - accuracy: 0.8909 - val_loss: 0.2561 - val_accuracy: 0.8824\n",
      "Epoch 116/150\n",
      "275/275 [==============================] - 0s 188us/sample - loss: 0.2476 - accuracy: 0.8909 - val_loss: 0.2563 - val_accuracy: 0.8824\n",
      "Epoch 117/150\n",
      "275/275 [==============================] - 0s 189us/sample - loss: 0.2472 - accuracy: 0.8945 - val_loss: 0.2566 - val_accuracy: 0.8824\n",
      "Epoch 118/150\n",
      "275/275 [==============================] - 0s 189us/sample - loss: 0.2469 - accuracy: 0.8945 - val_loss: 0.2567 - val_accuracy: 0.8824\n",
      "Epoch 119/150\n",
      "275/275 [==============================] - 0s 189us/sample - loss: 0.2466 - accuracy: 0.8945 - val_loss: 0.2569 - val_accuracy: 0.8824\n",
      "Epoch 120/150\n",
      "275/275 [==============================] - 0s 193us/sample - loss: 0.2464 - accuracy: 0.8945 - val_loss: 0.2572 - val_accuracy: 0.8824\n",
      "Epoch 121/150\n",
      "275/275 [==============================] - 0s 219us/sample - loss: 0.2460 - accuracy: 0.8945 - val_loss: 0.2574 - val_accuracy: 0.8824\n",
      "Epoch 122/150\n",
      "275/275 [==============================] - 0s 204us/sample - loss: 0.2458 - accuracy: 0.8945 - val_loss: 0.2577 - val_accuracy: 0.8824\n",
      "Epoch 123/150\n",
      "275/275 [==============================] - 0s 196us/sample - loss: 0.2455 - accuracy: 0.8945 - val_loss: 0.2578 - val_accuracy: 0.8824\n",
      "Epoch 124/150\n",
      "275/275 [==============================] - 0s 198us/sample - loss: 0.2452 - accuracy: 0.8945 - val_loss: 0.2579 - val_accuracy: 0.8824\n",
      "Epoch 125/150\n",
      "275/275 [==============================] - 0s 193us/sample - loss: 0.2450 - accuracy: 0.8945 - val_loss: 0.2582 - val_accuracy: 0.8824\n",
      "Epoch 126/150\n",
      "275/275 [==============================] - 0s 202us/sample - loss: 0.2447 - accuracy: 0.8945 - val_loss: 0.2582 - val_accuracy: 0.8824\n",
      "Epoch 127/150\n",
      "275/275 [==============================] - 0s 208us/sample - loss: 0.2445 - accuracy: 0.8945 - val_loss: 0.2581 - val_accuracy: 0.8824\n",
      "Epoch 128/150\n",
      "275/275 [==============================] - 0s 287us/sample - loss: 0.2442 - accuracy: 0.8945 - val_loss: 0.2584 - val_accuracy: 0.8824\n",
      "Epoch 129/150\n",
      "275/275 [==============================] - 0s 234us/sample - loss: 0.2440 - accuracy: 0.8945 - val_loss: 0.2588 - val_accuracy: 0.8824\n",
      "Epoch 130/150\n",
      "275/275 [==============================] - 0s 243us/sample - loss: 0.2436 - accuracy: 0.8945 - val_loss: 0.2589 - val_accuracy: 0.8824\n",
      "Epoch 131/150\n",
      "275/275 [==============================] - 0s 208us/sample - loss: 0.2434 - accuracy: 0.8945 - val_loss: 0.2590 - val_accuracy: 0.8824\n",
      "Epoch 132/150\n",
      "275/275 [==============================] - 0s 258us/sample - loss: 0.2432 - accuracy: 0.8945 - val_loss: 0.2594 - val_accuracy: 0.8824\n",
      "Epoch 133/150\n",
      "275/275 [==============================] - 0s 211us/sample - loss: 0.2429 - accuracy: 0.8945 - val_loss: 0.2595 - val_accuracy: 0.8824\n",
      "Epoch 134/150\n",
      "275/275 [==============================] - 0s 207us/sample - loss: 0.2427 - accuracy: 0.8945 - val_loss: 0.2596 - val_accuracy: 0.8824\n",
      "Epoch 135/150\n",
      "275/275 [==============================] - 0s 234us/sample - loss: 0.2425 - accuracy: 0.8945 - val_loss: 0.2598 - val_accuracy: 0.8824\n",
      "Epoch 136/150\n",
      "275/275 [==============================] - 0s 244us/sample - loss: 0.2422 - accuracy: 0.8945 - val_loss: 0.2600 - val_accuracy: 0.8824\n",
      "Epoch 137/150\n",
      "275/275 [==============================] - 0s 225us/sample - loss: 0.2421 - accuracy: 0.8945 - val_loss: 0.2600 - val_accuracy: 0.8824\n",
      "Epoch 138/150\n",
      "275/275 [==============================] - 0s 254us/sample - loss: 0.2420 - accuracy: 0.8945 - val_loss: 0.2605 - val_accuracy: 0.8824\n",
      "Epoch 139/150\n",
      "275/275 [==============================] - 0s 225us/sample - loss: 0.2417 - accuracy: 0.8945 - val_loss: 0.2605 - val_accuracy: 0.8824\n",
      "Epoch 140/150\n",
      "275/275 [==============================] - 0s 233us/sample - loss: 0.2415 - accuracy: 0.8945 - val_loss: 0.2608 - val_accuracy: 0.8824\n",
      "Epoch 141/150\n",
      "275/275 [==============================] - 0s 206us/sample - loss: 0.2412 - accuracy: 0.8945 - val_loss: 0.2609 - val_accuracy: 0.8824\n",
      "Epoch 142/150\n",
      "275/275 [==============================] - 0s 235us/sample - loss: 0.2409 - accuracy: 0.8945 - val_loss: 0.2611 - val_accuracy: 0.8824\n",
      "Epoch 143/150\n",
      "275/275 [==============================] - 0s 253us/sample - loss: 0.2408 - accuracy: 0.8945 - val_loss: 0.2612 - val_accuracy: 0.8824\n",
      "Epoch 144/150\n",
      "275/275 [==============================] - 0s 228us/sample - loss: 0.2405 - accuracy: 0.8945 - val_loss: 0.2614 - val_accuracy: 0.8824\n",
      "Epoch 145/150\n",
      "275/275 [==============================] - 0s 240us/sample - loss: 0.2403 - accuracy: 0.8945 - val_loss: 0.2614 - val_accuracy: 0.8824\n",
      "Epoch 146/150\n",
      "275/275 [==============================] - 0s 211us/sample - loss: 0.2402 - accuracy: 0.8945 - val_loss: 0.2616 - val_accuracy: 0.8824\n",
      "Epoch 147/150\n",
      "275/275 [==============================] - 0s 241us/sample - loss: 0.2403 - accuracy: 0.8945 - val_loss: 0.2620 - val_accuracy: 0.8824\n",
      "Epoch 148/150\n",
      "275/275 [==============================] - 0s 278us/sample - loss: 0.2399 - accuracy: 0.8945 - val_loss: 0.2621 - val_accuracy: 0.8824\n",
      "Epoch 149/150\n",
      "275/275 [==============================] - 0s 235us/sample - loss: 0.2396 - accuracy: 0.8945 - val_loss: 0.2622 - val_accuracy: 0.8824\n",
      "Epoch 150/150\n",
      "275/275 [==============================] - 0s 236us/sample - loss: 0.2395 - accuracy: 0.8945 - val_loss: 0.2623 - val_accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "model_simple = tf.keras.models.Sequential(name = 'ANN')\n",
    "\n",
    "model_simple.add(tf.keras.layers.Dense(6, activation='relu', input_shape=(9,), name='hidden1'))\n",
    "model_simple.add(tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model_simple.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model_simple.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/1 - 0s - loss: 0.1635 - accuracy: 0.9800\n",
      "Test accuracy=0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "test_loss1, test_accuracy1 = model_simple.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy={test_accuracy1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

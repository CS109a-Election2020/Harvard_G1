{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Desktop/cs209a-FinalProject/Harvard_G1/data/Merged_Fundamentals_and_Polls/Remastered_df_10pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>popular_vote_percentage</th>\n",
       "      <th>NAP</th>\n",
       "      <th>payroll</th>\n",
       "      <th>stock</th>\n",
       "      <th>rdi</th>\n",
       "      <th>election</th>\n",
       "      <th>national_polls</th>\n",
       "      <th>gdp</th>\n",
       "      <th>state_election</th>\n",
       "      <th>Democrat_Republican</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49.55</td>\n",
       "      <td>0.825641</td>\n",
       "      <td>0.234242</td>\n",
       "      <td>-9.718823</td>\n",
       "      <td>-1.423503</td>\n",
       "      <td>0.031317</td>\n",
       "      <td>1.083528</td>\n",
       "      <td>-0.089281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>45.07</td>\n",
       "      <td>0.825641</td>\n",
       "      <td>0.234242</td>\n",
       "      <td>-9.718823</td>\n",
       "      <td>-1.423503</td>\n",
       "      <td>0.031317</td>\n",
       "      <td>0.377789</td>\n",
       "      <td>-0.089281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>55.18</td>\n",
       "      <td>-0.310126</td>\n",
       "      <td>0.261147</td>\n",
       "      <td>-1.243207</td>\n",
       "      <td>9.327366</td>\n",
       "      <td>0.115117</td>\n",
       "      <td>-0.339976</td>\n",
       "      <td>0.243113</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44.33</td>\n",
       "      <td>-0.310126</td>\n",
       "      <td>0.261147</td>\n",
       "      <td>-1.243207</td>\n",
       "      <td>9.327366</td>\n",
       "      <td>0.115117</td>\n",
       "      <td>0.145107</td>\n",
       "      <td>0.243113</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>57.37</td>\n",
       "      <td>0.277996</td>\n",
       "      <td>0.160302</td>\n",
       "      <td>-16.387651</td>\n",
       "      <td>6.012349</td>\n",
       "      <td>0.055845</td>\n",
       "      <td>0.396180</td>\n",
       "      <td>0.158822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  popular_vote_percentage       NAP   payroll      stock       rdi  election  national_polls       gdp  state_election  Democrat_Republican\n",
       "0           0                    49.55  0.825641  0.234242  -9.718823 -1.423503  0.031317        1.083528 -0.089281             0.0                    1\n",
       "1           1                    45.07  0.825641  0.234242  -9.718823 -1.423503  0.031317        0.377789 -0.089281             0.0                    0\n",
       "2           2                    55.18 -0.310126  0.261147  -1.243207  9.327366  0.115117       -0.339976  0.243113            -1.0                    0\n",
       "3           3                    44.33 -0.310126  0.261147  -1.243207  9.327366  0.115117        0.145107  0.243113            -1.0                    1\n",
       "4           4                    57.37  0.277996  0.160302 -16.387651  6.012349  0.055845        0.396180  0.158822             1.0                    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Unnamed: 0', 'popular_vote_percentage'], axis=1)\n",
    "Y = df['popular_vote_percentage']\n",
    "X_train = X.drop([36,37]).values\n",
    "y_train = Y.drop([36,37]).values\n",
    "X_test = X.iloc[[36,37]].values\n",
    "y_test = Y.iloc[[36,37]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAP</th>\n",
       "      <th>payroll</th>\n",
       "      <th>stock</th>\n",
       "      <th>rdi</th>\n",
       "      <th>election</th>\n",
       "      <th>national_polls</th>\n",
       "      <th>gdp</th>\n",
       "      <th>Democrat_Republican</th>\n",
       "      <th>state_election</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.317666</td>\n",
       "      <td>0.694822</td>\n",
       "      <td>-0.191417</td>\n",
       "      <td>-0.443721</td>\n",
       "      <td>-0.043218</td>\n",
       "      <td>2.009872</td>\n",
       "      <td>-2.811254</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.317666</td>\n",
       "      <td>0.694822</td>\n",
       "      <td>-0.191417</td>\n",
       "      <td>-0.443721</td>\n",
       "      <td>-0.043218</td>\n",
       "      <td>0.271941</td>\n",
       "      <td>-2.811254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.618086</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.393234</td>\n",
       "      <td>1.600343</td>\n",
       "      <td>1.549956</td>\n",
       "      <td>-1.495606</td>\n",
       "      <td>2.293957</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.618086</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.393234</td>\n",
       "      <td>1.600343</td>\n",
       "      <td>1.549956</td>\n",
       "      <td>-0.301056</td>\n",
       "      <td>2.293957</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.419923</td>\n",
       "      <td>0.360434</td>\n",
       "      <td>-0.651435</td>\n",
       "      <td>0.970059</td>\n",
       "      <td>0.423092</td>\n",
       "      <td>0.317229</td>\n",
       "      <td>0.999344</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.419923</td>\n",
       "      <td>0.360434</td>\n",
       "      <td>-0.651435</td>\n",
       "      <td>0.970059</td>\n",
       "      <td>0.423092</td>\n",
       "      <td>-0.326338</td>\n",
       "      <td>0.999344</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>-0.169566</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>-1.287107</td>\n",
       "      <td>0.063623</td>\n",
       "      <td>0.379243</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>-0.169566</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>-1.287107</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>0.379243</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.744231</td>\n",
       "      <td>0.323772</td>\n",
       "      <td>-0.014344</td>\n",
       "      <td>0.312956</td>\n",
       "      <td>0.338592</td>\n",
       "      <td>-0.900030</td>\n",
       "      <td>0.806024</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.744231</td>\n",
       "      <td>0.323772</td>\n",
       "      <td>-0.014344</td>\n",
       "      <td>0.312956</td>\n",
       "      <td>0.338592</td>\n",
       "      <td>3.529049</td>\n",
       "      <td>0.806024</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.310866</td>\n",
       "      <td>0.606668</td>\n",
       "      <td>-0.107557</td>\n",
       "      <td>-0.252279</td>\n",
       "      <td>0.198035</td>\n",
       "      <td>0.906873</td>\n",
       "      <td>0.072359</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.310866</td>\n",
       "      <td>0.606668</td>\n",
       "      <td>-0.107557</td>\n",
       "      <td>-0.252279</td>\n",
       "      <td>0.198035</td>\n",
       "      <td>0.149001</td>\n",
       "      <td>0.072359</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.034297</td>\n",
       "      <td>0.369203</td>\n",
       "      <td>0.711918</td>\n",
       "      <td>-0.081166</td>\n",
       "      <td>1.107026</td>\n",
       "      <td>-0.225820</td>\n",
       "      <td>-0.124084</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.034297</td>\n",
       "      <td>0.369203</td>\n",
       "      <td>0.711918</td>\n",
       "      <td>-0.081166</td>\n",
       "      <td>1.107026</td>\n",
       "      <td>-0.456783</td>\n",
       "      <td>-0.124084</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.578420</td>\n",
       "      <td>-0.003607</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>-1.481712</td>\n",
       "      <td>0.122942</td>\n",
       "      <td>1.019268</td>\n",
       "      <td>-0.954256</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.578420</td>\n",
       "      <td>-0.003607</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>-1.481712</td>\n",
       "      <td>0.122942</td>\n",
       "      <td>-1.567691</td>\n",
       "      <td>-0.954256</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.268752</td>\n",
       "      <td>0.154460</td>\n",
       "      <td>0.631563</td>\n",
       "      <td>-0.675783</td>\n",
       "      <td>-0.011863</td>\n",
       "      <td>2.261010</td>\n",
       "      <td>-0.022789</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.268752</td>\n",
       "      <td>0.154460</td>\n",
       "      <td>0.631563</td>\n",
       "      <td>-0.675783</td>\n",
       "      <td>-0.011863</td>\n",
       "      <td>0.168908</td>\n",
       "      <td>-0.022789</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.502108</td>\n",
       "      <td>0.426535</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>0.589878</td>\n",
       "      <td>0.524624</td>\n",
       "      <td>-0.135038</td>\n",
       "      <td>0.765058</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.502108</td>\n",
       "      <td>0.426535</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>0.589878</td>\n",
       "      <td>0.524624</td>\n",
       "      <td>-0.986547</td>\n",
       "      <td>0.765058</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.150340</td>\n",
       "      <td>0.442678</td>\n",
       "      <td>1.220296</td>\n",
       "      <td>0.365692</td>\n",
       "      <td>0.593960</td>\n",
       "      <td>1.295891</td>\n",
       "      <td>0.382612</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.150340</td>\n",
       "      <td>0.442678</td>\n",
       "      <td>1.220296</td>\n",
       "      <td>0.365692</td>\n",
       "      <td>0.593960</td>\n",
       "      <td>-2.006252</td>\n",
       "      <td>0.382612</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.478517</td>\n",
       "      <td>-0.296785</td>\n",
       "      <td>-0.549464</td>\n",
       "      <td>2.372707</td>\n",
       "      <td>0.538290</td>\n",
       "      <td>0.147710</td>\n",
       "      <td>-0.989096</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.478517</td>\n",
       "      <td>-0.296785</td>\n",
       "      <td>-0.549464</td>\n",
       "      <td>2.372707</td>\n",
       "      <td>0.538290</td>\n",
       "      <td>-0.194180</td>\n",
       "      <td>-0.989096</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.314950</td>\n",
       "      <td>0.316662</td>\n",
       "      <td>2.393224</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.611786</td>\n",
       "      <td>-0.342111</td>\n",
       "      <td>-0.132163</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.314950</td>\n",
       "      <td>0.316662</td>\n",
       "      <td>2.393224</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.611786</td>\n",
       "      <td>-0.337024</td>\n",
       "      <td>-0.132163</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.925613</td>\n",
       "      <td>0.205943</td>\n",
       "      <td>0.279646</td>\n",
       "      <td>0.066245</td>\n",
       "      <td>0.115579</td>\n",
       "      <td>-0.163542</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.925613</td>\n",
       "      <td>0.205943</td>\n",
       "      <td>0.279646</td>\n",
       "      <td>0.066245</td>\n",
       "      <td>0.115579</td>\n",
       "      <td>-0.209770</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.163355</td>\n",
       "      <td>-0.168767</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>-0.063217</td>\n",
       "      <td>0.437028</td>\n",
       "      <td>-0.353588</td>\n",
       "      <td>0.210672</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.163355</td>\n",
       "      <td>-0.168767</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>-0.063217</td>\n",
       "      <td>0.437028</td>\n",
       "      <td>-0.510705</td>\n",
       "      <td>0.210672</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-2.062588</td>\n",
       "      <td>-0.437901</td>\n",
       "      <td>-2.811646</td>\n",
       "      <td>-0.371880</td>\n",
       "      <td>-1.980385</td>\n",
       "      <td>0.361135</td>\n",
       "      <td>-0.435877</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-2.062588</td>\n",
       "      <td>-0.437901</td>\n",
       "      <td>-2.811646</td>\n",
       "      <td>-0.371880</td>\n",
       "      <td>-1.980385</td>\n",
       "      <td>-0.494657</td>\n",
       "      <td>-0.435877</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.666081</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>0.446411</td>\n",
       "      <td>-1.959467</td>\n",
       "      <td>-0.440551</td>\n",
       "      <td>-0.456966</td>\n",
       "      <td>-0.550994</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.666081</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>0.446411</td>\n",
       "      <td>-1.959467</td>\n",
       "      <td>-0.440551</td>\n",
       "      <td>-0.261482</td>\n",
       "      <td>-0.550994</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.323229</td>\n",
       "      <td>0.158559</td>\n",
       "      <td>-1.201586</td>\n",
       "      <td>-1.252343</td>\n",
       "      <td>-0.013384</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.333316</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.323229</td>\n",
       "      <td>0.158559</td>\n",
       "      <td>-1.201586</td>\n",
       "      <td>-1.252343</td>\n",
       "      <td>-0.013384</td>\n",
       "      <td>-0.093528</td>\n",
       "      <td>-0.333316</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.396071</td>\n",
       "      <td>-4.028853</td>\n",
       "      <td>-0.532175</td>\n",
       "      <td>-0.471771</td>\n",
       "      <td>-2.784402</td>\n",
       "      <td>-0.316497</td>\n",
       "      <td>-0.341324</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.396071</td>\n",
       "      <td>-4.028853</td>\n",
       "      <td>-0.532175</td>\n",
       "      <td>-0.471771</td>\n",
       "      <td>-2.784402</td>\n",
       "      <td>-0.371412</td>\n",
       "      <td>-0.341324</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NAP   payroll     stock       rdi  election  national_polls       gdp  Democrat_Republican  state_election\n",
       "0   2.317666  0.694822 -0.191417 -0.443721 -0.043218        2.009872 -2.811254                    1             0.0\n",
       "1   2.317666  0.694822 -0.191417 -0.443721 -0.043218        0.271941 -2.811254                    0             0.0\n",
       "2  -1.618086  0.816498  0.393234  1.600343  1.549956       -1.495606  2.293957                    0            -1.0\n",
       "3  -1.618086  0.816498  0.393234  1.600343  1.549956       -0.301056  2.293957                    1            -1.0\n",
       "4   0.419923  0.360434 -0.651435  0.970059  0.423092        0.317229  0.999344                    0             1.0\n",
       "5   0.419923  0.360434 -0.651435  0.970059  0.423092       -0.326338  0.999344                    1             1.0\n",
       "6   0.011446  0.036944 -0.169566  0.769470 -1.287107        0.063623  0.379243                    1            -1.0\n",
       "7   0.011446  0.036944 -0.169566  0.769470 -1.287107        0.013565  0.379243                    0            -1.0\n",
       "8  -0.744231  0.323772 -0.014344  0.312956  0.338592       -0.900030  0.806024                    1             1.0\n",
       "9  -0.744231  0.323772 -0.014344  0.312956  0.338592        3.529049  0.806024                    0             1.0\n",
       "10 -0.310866  0.606668 -0.107557 -0.252279  0.198035        0.906873  0.072359                    1             0.0\n",
       "11 -0.310866  0.606668 -0.107557 -0.252279  0.198035        0.149001  0.072359                    0             0.0\n",
       "12  0.034297  0.369203  0.711918 -0.081166  1.107026       -0.225820 -0.124084                    1             1.0\n",
       "13  0.034297  0.369203  0.711918 -0.081166  1.107026       -0.456783 -0.124084                    0             1.0\n",
       "14  0.578420 -0.003607  0.084509 -1.481712  0.122942        1.019268 -0.954256                    0             1.0\n",
       "15  0.578420 -0.003607  0.084509 -1.481712  0.122942       -1.567691 -0.954256                    1             1.0\n",
       "16  1.268752  0.154460  0.631563 -0.675783 -0.011863        2.261010 -0.022789                    1             1.0\n",
       "17  1.268752  0.154460  0.631563 -0.675783 -0.011863        0.168908 -0.022789                    0             1.0\n",
       "18 -0.502108  0.426535  0.023123  0.589878  0.524624       -0.135038  0.765058                    0             1.0\n",
       "19 -0.502108  0.426535  0.023123  0.589878  0.524624       -0.986547  0.765058                    1             1.0\n",
       "20 -0.150340  0.442678  1.220296  0.365692  0.593960        1.295891  0.382612                    0            -1.0\n",
       "21 -0.150340  0.442678  1.220296  0.365692  0.593960       -2.006252  0.382612                    1            -1.0\n",
       "22  0.478517 -0.296785 -0.549464  2.372707  0.538290        0.147710 -0.989096                    1             1.0\n",
       "23  0.478517 -0.296785 -0.549464  2.372707  0.538290       -0.194180 -0.989096                    0             1.0\n",
       "24  0.314950  0.316662  2.393224  0.005989  0.611786       -0.342111 -0.132163                    1             1.0\n",
       "25  0.314950  0.316662  2.393224  0.005989  0.611786       -0.337024 -0.132163                    0             1.0\n",
       "26 -0.925613  0.205943  0.279646  0.066245  0.115579       -0.163542  0.785885                    1            -1.0\n",
       "27 -0.925613  0.205943  0.279646  0.066245  0.115579       -0.209770  0.785885                    0            -1.0\n",
       "28 -0.163355 -0.168767  0.045264 -0.063217  0.437028       -0.353588  0.210672                    0             1.0\n",
       "29 -0.163355 -0.168767  0.045264 -0.063217  0.437028       -0.510705  0.210672                    1             1.0\n",
       "30 -2.062588 -0.437901 -2.811646 -0.371880 -1.980385        0.361135 -0.435877                    1            -1.0\n",
       "31 -2.062588 -0.437901 -2.811646 -0.371880 -1.980385       -0.494657 -0.435877                    0            -1.0\n",
       "32 -0.666081  0.022733  0.446411 -1.959467 -0.440551       -0.456966 -0.550994                    1             1.0\n",
       "33 -0.666081  0.022733  0.446411 -1.959467 -0.440551       -0.261482 -0.550994                    0             1.0\n",
       "34  0.323229  0.158559 -1.201586 -1.252343 -0.013384       -0.008449 -0.333316                    0            -1.0\n",
       "35  0.323229  0.158559 -1.201586 -1.252343 -0.013384       -0.093528 -0.333316                    1            -1.0\n",
       "36  1.396071 -4.028853 -0.532175 -0.471771 -2.784402       -0.316497 -0.341324                    0             1.0\n",
       "37  1.396071 -4.028853 -0.532175 -0.471771 -2.784402       -0.371412 -0.341324                    1             1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_1 = df.drop(['Unnamed: 0', 'popular_vote_percentage', 'Democrat_Republican', 'state_election'], axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "x_2 = scaler.transform(X_1)\n",
    "X_2 = pd.DataFrame(x_2, columns = ['NAP', 'payroll','stock','rdi','election','national_polls','gdp'])\n",
    "X_2['Democrat_Republican'] = df['Democrat_Republican']\n",
    "X_2['state_election'] = df['state_election']\n",
    "X_train1 = X_2.drop([36,37]).values\n",
    "X_test1 = X_2.iloc[[36,37]].values\n",
    "display(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV Model\n",
    "##### Not Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 412.9960240740985, tolerance: 0.10804503428571427\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 277.0987926113276, tolerance: 0.07136670689655174\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 392.69456685571123, tolerance: 0.10238491034482758\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 437.2294216024677, tolerance: 0.1090733696551724\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479.60394671227766, tolerance: 0.12473298137931034\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# List of hyper-parameter values \n",
    "\n",
    "alphas = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,0,1,10,100,1000]\n",
    "\n",
    "# Create two lists for training and validation error\n",
    "\n",
    "training_error, validation_error = [],[]\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    \n",
    "    # For each i, fit a ridge regression on training set\n",
    "    \n",
    "    lasso = Lasso(fit_intercept=True, alpha=alpha)\n",
    "    #lasso.fit(X_train,y_train)\n",
    "    lasso_cv = cross_validate(lasso, X_train, y_train, cv = 5, scoring=('neg_mean_squared_error'))\n",
    "    \n",
    "    # Compute the training and validation errors got after cross validation\n",
    "    mse_val = np.mean(np.abs(lasso_cv['test_score']))\n",
    "        \n",
    "    # Append the MSEs to their respective lists \n",
    "    validation_error.append(mse_val)\n",
    "\n",
    "best_mse  = np.min(validation_error)\n",
    "best_parameter = alphas[np.argmin(validation_error)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 37.20568370440923, best alpha: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {best_mse}, best alpha: {best_parameter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MSE: 4.524136111111125\n",
      "Lasso R2: -0.5654450211457194\n",
      "[47.82166667 47.82166667]\n",
      "[-0.  0. -0. -0. -0. -0.  0. -0. -0.]\n",
      "47.82166666666666\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(fit_intercept=True, alpha=10).fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lasso)}\")\n",
    "print(f\"Lasso R2: {r2_score(y_test, y_pred_lasso)}\")\n",
    "print(y_pred_lasso)\n",
    "print(lasso.coef_)\n",
    "print(lasso.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV Model\n",
    "##### Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 412.99602407409793, tolerance: 0.10804503428571427\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 277.0987926113273, tolerance: 0.07136670689655174\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 392.6945668557105, tolerance: 0.10238491034482758\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 437.2294216024675, tolerance: 0.1090733696551724\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/talelokvenec/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479.60394671227454, tolerance: 0.12473298137931034\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# List of hyper-parameter values \n",
    "\n",
    "alphas = [1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,0,1,10,100,1000]\n",
    "\n",
    "# Create two lists for training and validation error\n",
    "\n",
    "validation_error1 = []\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    \n",
    "    # For each i, fit a ridge regression on training set\n",
    "    \n",
    "    lasso = Lasso(fit_intercept=True, alpha=alpha)\n",
    "    #lasso.fit(X_train,y_train)\n",
    "    lasso_cv1 = cross_validate(lasso, X_train1, y_train, cv = 5, scoring=('neg_mean_squared_error'))\n",
    "    \n",
    "    # Compute the training and validation errors got after cross validation\n",
    "    mse_val = np.mean(np.abs(lasso_cv1['test_score']))\n",
    "        \n",
    "    # Append the MSEs to their respective lists \n",
    "    validation_error1.append(mse_val)\n",
    "\n",
    "best_mse1  = np.min(validation_error1)\n",
    "best_parameter1 = alphas[np.argmin(validation_error1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 37.20568370440923, best alpha: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {best_mse1}, best alpha: {best_parameter1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MSE: 4.524136111111125\n",
      "Lasso R2: -0.5654450211457194\n",
      "[47.82166667 47.82166667]\n",
      "[-0.  0. -0. -0. -0. -0.  0. -0. -0.]\n",
      "47.82166666666666\n"
     ]
    }
   ],
   "source": [
    "lasso1 = Lasso(fit_intercept=True, alpha=10).fit(X_train1, y_train)\n",
    "y_pred_lasso1 = lasso1.predict(X_test1)\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lasso1)}\")\n",
    "print(f\"Lasso R2: {r2_score(y_test, y_pred_lasso1)}\")\n",
    "print(y_pred_lasso1)\n",
    "print(lasso1.coef_)\n",
    "print(lasso1.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tried once more using LassoCV model, in case I was doing something wrong above, still the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MSE: 4.524136111111125\n",
      "Lasso R2: -0.5654450211457194\n",
      "[47.82166667 47.82166667]\n",
      "[-0.  0. -0. -0. -0. -0.  0. -0. -0.]\n",
      "47.82166666666666\n"
     ]
    }
   ],
   "source": [
    "lassoCV = LassoCV(fit_intercept = True, cv = 5).fit(X_train1, y_train)\n",
    "y_pred_lassoCV = lassoCV.predict(X_test1)\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lassoCV)}\")\n",
    "print(f\"Lasso R2: {r2_score(y_test, y_pred_lassoCV)}\")\n",
    "print(y_pred_lassoCV)\n",
    "print(lassoCV.coef_)\n",
    "print(lassoCV.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "#### Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31236123 0.2226887  0.13932112 0.11166227 0.08443899 0.0622966\n",
      " 0.03024289 0.02505786 0.01193034]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "      <th>PCA4</th>\n",
       "      <th>PCA5</th>\n",
       "      <th>PCA6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.316111</td>\n",
       "      <td>-2.372364</td>\n",
       "      <td>-2.244506</td>\n",
       "      <td>0.411688</td>\n",
       "      <td>-1.253257</td>\n",
       "      <td>0.479474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.055887</td>\n",
       "      <td>-1.975623</td>\n",
       "      <td>-1.187277</td>\n",
       "      <td>-0.365806</td>\n",
       "      <td>-2.058805</td>\n",
       "      <td>0.337249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.810777</td>\n",
       "      <td>1.391599</td>\n",
       "      <td>0.691553</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>-0.323877</td>\n",
       "      <td>0.427886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.633294</td>\n",
       "      <td>1.122014</td>\n",
       "      <td>-0.020166</td>\n",
       "      <td>0.563587</td>\n",
       "      <td>0.205593</td>\n",
       "      <td>0.517815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.696461</td>\n",
       "      <td>-0.112131</td>\n",
       "      <td>0.051944</td>\n",
       "      <td>1.287707</td>\n",
       "      <td>0.052392</td>\n",
       "      <td>-0.692427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.798868</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.508995</td>\n",
       "      <td>0.934048</td>\n",
       "      <td>-0.352038</td>\n",
       "      <td>-0.779402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.069391</td>\n",
       "      <td>1.226276</td>\n",
       "      <td>-0.301185</td>\n",
       "      <td>0.505179</td>\n",
       "      <td>-0.366204</td>\n",
       "      <td>1.177345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.066180</td>\n",
       "      <td>1.228040</td>\n",
       "      <td>-0.317193</td>\n",
       "      <td>0.529383</td>\n",
       "      <td>-0.314186</td>\n",
       "      <td>1.197564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.095088</td>\n",
       "      <td>0.344331</td>\n",
       "      <td>0.854294</td>\n",
       "      <td>-0.190131</td>\n",
       "      <td>0.107043</td>\n",
       "      <td>-0.870348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.416260</td>\n",
       "      <td>-0.702062</td>\n",
       "      <td>-2.009782</td>\n",
       "      <td>1.961555</td>\n",
       "      <td>2.434797</td>\n",
       "      <td>-0.419048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.302026</td>\n",
       "      <td>-0.131808</td>\n",
       "      <td>-1.049748</td>\n",
       "      <td>-0.023188</td>\n",
       "      <td>0.416008</td>\n",
       "      <td>-0.027420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.413016</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>-0.615691</td>\n",
       "      <td>-0.335178</td>\n",
       "      <td>0.108403</td>\n",
       "      <td>-0.075322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.721834</td>\n",
       "      <td>-1.172802</td>\n",
       "      <td>0.411793</td>\n",
       "      <td>-0.323621</td>\n",
       "      <td>-0.022702</td>\n",
       "      <td>-0.492316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.752591</td>\n",
       "      <td>-1.128705</td>\n",
       "      <td>0.510813</td>\n",
       "      <td>-0.385341</td>\n",
       "      <td>-0.062597</td>\n",
       "      <td>-0.489508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.331455</td>\n",
       "      <td>-1.300415</td>\n",
       "      <td>-0.743972</td>\n",
       "      <td>-0.556015</td>\n",
       "      <td>0.715952</td>\n",
       "      <td>-0.430290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.933127</td>\n",
       "      <td>-0.685093</td>\n",
       "      <td>0.948789</td>\n",
       "      <td>-1.832739</td>\n",
       "      <td>-0.675866</td>\n",
       "      <td>-0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.938696</td>\n",
       "      <td>-1.845492</td>\n",
       "      <td>-0.960927</td>\n",
       "      <td>0.767399</td>\n",
       "      <td>1.305400</td>\n",
       "      <td>0.318660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.624543</td>\n",
       "      <td>-1.365872</td>\n",
       "      <td>0.321502</td>\n",
       "      <td>-0.178317</td>\n",
       "      <td>0.319906</td>\n",
       "      <td>0.142349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.107531</td>\n",
       "      <td>-0.064890</td>\n",
       "      <td>0.327019</td>\n",
       "      <td>0.436334</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>-0.726909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.241602</td>\n",
       "      <td>0.144321</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>-0.016094</td>\n",
       "      <td>-0.187818</td>\n",
       "      <td>-0.833897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.107038</td>\n",
       "      <td>-0.581188</td>\n",
       "      <td>-1.038482</td>\n",
       "      <td>0.404292</td>\n",
       "      <td>0.600246</td>\n",
       "      <td>1.533433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.614267</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>1.109029</td>\n",
       "      <td>-1.212127</td>\n",
       "      <td>-1.154940</td>\n",
       "      <td>1.190592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.067254</td>\n",
       "      <td>-0.454931</td>\n",
       "      <td>0.295715</td>\n",
       "      <td>2.074840</td>\n",
       "      <td>-1.561966</td>\n",
       "      <td>-0.749289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.114902</td>\n",
       "      <td>-0.384876</td>\n",
       "      <td>0.465269</td>\n",
       "      <td>1.960431</td>\n",
       "      <td>-1.658222</td>\n",
       "      <td>-0.757156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.789964</td>\n",
       "      <td>-1.932977</td>\n",
       "      <td>1.155434</td>\n",
       "      <td>-0.491958</td>\n",
       "      <td>0.316695</td>\n",
       "      <td>0.594592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.784778</td>\n",
       "      <td>-1.944117</td>\n",
       "      <td>1.104361</td>\n",
       "      <td>-0.441561</td>\n",
       "      <td>0.396731</td>\n",
       "      <td>0.620118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.135942</td>\n",
       "      <td>0.959686</td>\n",
       "      <td>-0.269294</td>\n",
       "      <td>-0.428988</td>\n",
       "      <td>0.220031</td>\n",
       "      <td>0.810115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.138570</td>\n",
       "      <td>0.960553</td>\n",
       "      <td>-0.287738</td>\n",
       "      <td>-0.402964</td>\n",
       "      <td>0.273994</td>\n",
       "      <td>0.830703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.239997</td>\n",
       "      <td>-0.223749</td>\n",
       "      <td>0.579857</td>\n",
       "      <td>-0.043928</td>\n",
       "      <td>0.232591</td>\n",
       "      <td>-0.623524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.268333</td>\n",
       "      <td>-0.177032</td>\n",
       "      <td>0.727598</td>\n",
       "      <td>-0.166536</td>\n",
       "      <td>0.075313</td>\n",
       "      <td>-0.663682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.291720</td>\n",
       "      <td>3.570488</td>\n",
       "      <td>-1.693195</td>\n",
       "      <td>-0.203538</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>-0.752705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.165820</td>\n",
       "      <td>3.760801</td>\n",
       "      <td>-1.196876</td>\n",
       "      <td>-0.562037</td>\n",
       "      <td>-0.346017</td>\n",
       "      <td>-0.810031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.806472</td>\n",
       "      <td>-0.246866</td>\n",
       "      <td>0.284101</td>\n",
       "      <td>-1.893718</td>\n",
       "      <td>0.865898</td>\n",
       "      <td>-0.630702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.840650</td>\n",
       "      <td>-0.302562</td>\n",
       "      <td>0.111964</td>\n",
       "      <td>-1.752886</td>\n",
       "      <td>1.042669</td>\n",
       "      <td>-0.586851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.747223</td>\n",
       "      <td>0.697020</td>\n",
       "      <td>-1.216537</td>\n",
       "      <td>-1.002537</td>\n",
       "      <td>-0.416006</td>\n",
       "      <td>0.253422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.729856</td>\n",
       "      <td>0.726879</td>\n",
       "      <td>-1.114601</td>\n",
       "      <td>-1.090929</td>\n",
       "      <td>-0.536683</td>\n",
       "      <td>0.220197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.173017</td>\n",
       "      <td>1.332622</td>\n",
       "      <td>2.404048</td>\n",
       "      <td>1.046787</td>\n",
       "      <td>0.686636</td>\n",
       "      <td>0.746967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.160243</td>\n",
       "      <td>1.355423</td>\n",
       "      <td>2.486803</td>\n",
       "      <td>0.972722</td>\n",
       "      <td>0.581285</td>\n",
       "      <td>0.716645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PCA1      PCA2      PCA3      PCA4      PCA5      PCA6\n",
       "0   2.316111 -2.372364 -2.244506  0.411688 -1.253257  0.479474\n",
       "1   2.055887 -1.975623 -1.187277 -0.365806 -2.058805  0.337249\n",
       "2  -3.810777  1.391599  0.691553  0.044185 -0.323877  0.427886\n",
       "3  -3.633294  1.122014 -0.020166  0.563587  0.205593  0.517815\n",
       "4  -0.696461 -0.112131  0.051944  1.287707  0.052392 -0.692427\n",
       "5  -0.798868  0.048419  0.508995  0.934048 -0.352038 -0.779402\n",
       "6   0.069391  1.226276 -0.301185  0.505179 -0.366204  1.177345\n",
       "7   0.066180  1.228040 -0.317193  0.529383 -0.314186  1.197564\n",
       "8  -1.095088  0.344331  0.854294 -0.190131  0.107043 -0.870348\n",
       "9  -0.416260 -0.702062 -2.009782  1.961555  2.434797 -0.419048\n",
       "10 -0.302026 -0.131808 -1.049748 -0.023188  0.416008 -0.027420\n",
       "11 -0.413016  0.035590 -0.615691 -0.335178  0.108403 -0.075322\n",
       "12 -0.721834 -1.172802  0.411793 -0.323621 -0.022702 -0.492316\n",
       "13 -0.752591 -1.128705  0.510813 -0.385341 -0.062597 -0.489508\n",
       "14  1.331455 -1.300415 -0.743972 -0.556015  0.715952 -0.430290\n",
       "15  0.933127 -0.685093  0.948789 -1.832739 -0.675866 -0.704300\n",
       "16  0.938696 -1.845492 -0.960927  0.767399  1.305400  0.318660\n",
       "17  0.624543 -1.365872  0.321502 -0.178317  0.319906  0.142349\n",
       "18 -1.107531 -0.064890  0.327019  0.436334  0.322262 -0.726909\n",
       "19 -1.241602  0.144321  0.916291 -0.016094 -0.187818 -0.833897\n",
       "20 -1.107038 -0.581188 -1.038482  0.404292  0.600246  1.533433\n",
       "21 -1.614267  0.201493  1.109029 -1.212127 -1.154940  1.190592\n",
       "22 -0.067254 -0.454931  0.295715  2.074840 -1.561966 -0.749289\n",
       "23 -0.114902 -0.384876  0.465269  1.960431 -1.658222 -0.757156\n",
       "24 -0.789964 -1.932977  1.155434 -0.491958  0.316695  0.594592\n",
       "25 -0.784778 -1.944117  1.104361 -0.441561  0.396731  0.620118\n",
       "26 -1.135942  0.959686 -0.269294 -0.428988  0.220031  0.810115\n",
       "27 -1.138570  0.960553 -0.287738 -0.402964  0.273994  0.830703\n",
       "28 -0.239997 -0.223749  0.579857 -0.043928  0.232591 -0.623524\n",
       "29 -0.268333 -0.177032  0.727598 -0.166536  0.075313 -0.663682\n",
       "30  1.291720  3.570488 -1.693195 -0.203538  0.011339 -0.752705\n",
       "31  1.165820  3.760801 -1.196876 -0.562037 -0.346017 -0.810031\n",
       "32  0.806472 -0.246866  0.284101 -1.893718  0.865898 -0.630702\n",
       "33  0.840650 -0.302562  0.111964 -1.752886  1.042669 -0.586851\n",
       "34  0.747223  0.697020 -1.216537 -1.002537 -0.416006  0.253422\n",
       "35  0.729856  0.726879 -1.114601 -1.090929 -0.536683  0.220197\n",
       "36  4.173017  1.332622  2.404048  1.046787  0.686636  0.746967\n",
       "37  4.160243  1.355423  2.486803  0.972722  0.581285  0.716645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_pca = X_2.values\n",
    "pca = PCA().fit(X_pca)\n",
    "X_2_PCA = pca.transform(X_pca)\n",
    "print(pca.explained_variance_ratio_)\n",
    "df_PCA = pd.DataFrame(X_2_PCA[:,0:6], columns = [['PCA1' , 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6']])\n",
    "display(df_PCA)\n",
    "X_train_pca = df_PCA.drop([36,37]).values\n",
    "X_test_pca = df_PCA.iloc[[36,37]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression\n",
    "#### - Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 98.1840588708088\n",
      "R2: -32.97372279266743\n",
      "[40.64007287 38.5251679 ]\n"
     ]
    }
   ],
   "source": [
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train1, y_train)\n",
    "y_test_pred_lreg = lreg.predict(X_test1)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_test_pred_lreg)}\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred_lreg)}\")\n",
    "print(y_test_pred_lreg)\n",
    "lreg_initial_mse = mean_squared_error(y_test, y_test_pred_lreg)\n",
    "lreg_initial_r2 = r2_score(y_test, y_test_pred_lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 98.18405887080563\n",
      "R2: -32.97372279266634\n",
      "[40.64007287 38.5251679 ]\n"
     ]
    }
   ],
   "source": [
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train, y_train)\n",
    "y_test_pred_lreg1 = lreg.predict(X_test)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_test_pred_lreg1)}\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred_lreg1)}\")\n",
    "print(y_test_pred_lreg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.88000641758609\n",
      "R2: -0.6885835354969185\n",
      "[47.53195257 47.67868596]\n"
     ]
    }
   ],
   "source": [
    "lreg_pca = LinearRegression()\n",
    "lreg_pca.fit(X_train_pca, y_train)\n",
    "y_test_pred_lreg_pca = lreg_pca.predict(X_test_pca)\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_test_pred_lreg_pca)}\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred_lreg_pca)}\")\n",
    "print(y_test_pred_lreg_pca)\n",
    "lreg_pca_mse = mean_squared_error(y_test, y_test_pred_lreg_pca)\n",
    "lreg_pca_r2 = r2_score(y_test, y_test_pred_lreg_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression Initial Data</td>\n",
       "      <td>98.184059</td>\n",
       "      <td>-32.973723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression PCA</td>\n",
       "      <td>4.880006</td>\n",
       "      <td>-0.688584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        regressor        MSE         R2\n",
       "0  Linear Regression Initial Data  98.184059 -32.973723\n",
       "1           Linear Regression PCA   4.880006  -0.688584"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'regressor':['Linear Regression Initial Data', 'Linear Regression PCA'],\n",
    "                          'MSE':[lreg_initial_mse, lreg_pca_mse],\n",
    "                          'R2':[lreg_initial_r2, lreg_pca_r2]})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso Regression\n",
    "#### - Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping Lasso\n",
    "def bootstrap(df):\n",
    "    selectionIndex = np.random.randint(len(df), size = len(df))\n",
    "    new_df = df.iloc[selectionIndex]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_y = X_2.copy()\n",
    "X_2_y['popular_vote_percentage'] = df['popular_vote_percentage']\n",
    "X_2_y = X_2_y.drop([36,37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE WE NEED TO ADD THE CODE TO APPEND THE INTERCEPTS AND COEFFICIENTS FROM THE BOOTSTRAPS AND MAKE A HISTOGRAM OF THOSE\n",
    "n_bootstraps = 100\n",
    "coefs = []\n",
    "intercepts = []\n",
    "for n in range(n_bootstraps):\n",
    "    df_new = bootstrap(X_2_y)\n",
    "    #display(df_new)\n",
    "    X_train_bootstrap = df_new.drop(['popular_vote_percentage'], axis = 1).values\n",
    "    y_train_bootstrap = df_new['popular_vote_percentage'].values\n",
    "    lassoCV = LassoCV(fit_intercept = True, cv = 5).fit(X_train_bootstrap, y_train_bootstrap)\n",
    "    coefs.append(lassoCV.coef_)\n",
    "    intercepts.append(lassoCV.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1 = []\n",
    "coef2 = []\n",
    "coef3 = []\n",
    "coef4 = []\n",
    "coef5 = []\n",
    "coef6 = []\n",
    "coef7 = []\n",
    "coef8 = []\n",
    "coef9 = []\n",
    "for coef in coefs:\n",
    "    if float(coef[0]) >= 0.1:\n",
    "        coef1.append(coef[0])\n",
    "    else:\n",
    "        coef1.append(0)\n",
    "    if coef[1] >= 0.1:\n",
    "        coef2.append(coef[1])\n",
    "    else:\n",
    "        coef2.append(0)\n",
    "    if coef[2] >= 0.1:\n",
    "        coef3.append(coef[2])\n",
    "    else:\n",
    "        coef3.append(0)\n",
    "    if coef[3] >= 0.1:\n",
    "        coef4.append(coef[3])\n",
    "    else:\n",
    "        coef4.append(0)\n",
    "    if coef[4] >= 0.1:\n",
    "        coef5.append(coef[4])\n",
    "    else:\n",
    "        coef5.append(0)\n",
    "    if coef[5] >= 0.1:\n",
    "        coef6.append(coef[5])\n",
    "    else:\n",
    "        coef6.append(0)\n",
    "    if coef[6] >= 0.1:\n",
    "        coef7.append(coef[6])\n",
    "    else:\n",
    "        coef7.append(0)\n",
    "    if coef[7] >= 0.1:\n",
    "        coef8.append(coef[7])\n",
    "    else:\n",
    "        coef8.append(0)\n",
    "    if coef[8] >= 0.1:\n",
    "        coef9.append(coef[8])\n",
    "    else:\n",
    "        coef9.append(0)\n",
    "    #coef2.append(coef[1])\n",
    "    #coef3.append(coef[2])\n",
    "    #coef4.append(coef[3])\n",
    "    #coef5.append(coef[4])\n",
    "    #coef6.append(coef[5])\n",
    "    #coef7.append(coef[6])\n",
    "    #coef8.append(coef[7])\n",
    "    #coef9.append(coef[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame((coef1,coef2,coef3,coef4,coef5,coef6,coef7,coef8,coef9), index = ['coef1', 'coef2', 'coef3', 'coef4', 'coef5', 'coef6', 'coef7', 'coef8', 'coef9'])\n",
    "coefs = coefs.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(20, 20), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.2)\n",
    "axs = axs.ravel()\n",
    "fontsize = 10\n",
    "\n",
    "variables = ['coef1', 'coef2', 'coef3', 'coef4', 'coef5', 'coef6', 'coef7', 'coef8', 'coef9']\n",
    "\n",
    "# plot histograms\n",
    "for i, variable in enumerate(variables):\n",
    "    axs[i].hist(coefs[variable], alpha=0.7, color='royalblue', bins='auto', density=False, histtype = 'bar', edgecolor='k')\n",
    "    axs[i].set_title(\"Distribution of \" + variable, fontsize=fontsize)\n",
    "    axs[i].set_xlabel(variable, fontsize=fontsize)\n",
    "    axs[i].set_ylabel('Count', fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MSE: 4.524136111111125\n",
      "Lasso R2: -0.5654450211457194\n",
      "[47.82166667 47.82166667]\n",
      "[-0.  0. -0. -0. -0. -0.  0. -0. -0.]\n",
      "47.82166666666666\n"
     ]
    }
   ],
   "source": [
    "lassoCV = LassoCV(fit_intercept = True, cv = 10).fit(X_train1, y_train)\n",
    "y_pred_lassoCV = lassoCV.predict(X_test1)\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lassoCV)}\")\n",
    "print(f\"Lasso R2: {r2_score(y_test, y_pred_lassoCV)}\")\n",
    "print(y_pred_lassoCV)\n",
    "print(lassoCV.coef_)\n",
    "print(lassoCV.intercept_)\n",
    "lasso_initial_standardized_mse = mean_squared_error(y_test, y_pred_lassoCV)\n",
    "lasso_initial_standardized_r2 = r2_score(y_test, y_pred_lassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MSE: 4.524136111111125\n",
      "Lasso R2: -0.5654450211457194\n",
      "[47.82166667 47.82166667]\n",
      "[-0.00000000e+00  0.00000000e+00 -0.00000000e+00 -4.10642077e-17\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00]\n",
      "47.82166666666666\n"
     ]
    }
   ],
   "source": [
    "lassoCV1 = LassoCV(fit_intercept = True, cv = 10).fit(X_train, y_train)\n",
    "y_pred_lassoCV1 = lassoCV1.predict(X_test)\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lassoCV1)}\")\n",
    "print(f\"Lasso R2: {r2_score(y_test, y_pred_lassoCV1)}\")\n",
    "print(y_pred_lassoCV1)\n",
    "print(lassoCV1.coef_)\n",
    "print(lassoCV1.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso MSE: 4.524136111111125\n",
      "Lasso R2: -0.5654450211457194\n",
      "[47.82166667 47.82166667]\n",
      "[-0.00000000e+00  1.10123478e-16  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00]\n",
      "47.82166666666666\n"
     ]
    }
   ],
   "source": [
    "lassoCV_pca = LassoCV(fit_intercept = True, cv = 10).fit(X_train_pca, y_train)\n",
    "y_pred_lassoCV_pca = lassoCV_pca.predict(X_test_pca)\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lassoCV_pca)}\")\n",
    "print(f\"Lasso R2: {r2_score(y_test, y_pred_lassoCV_pca)}\")\n",
    "print(y_pred_lassoCV_pca)\n",
    "print(lassoCV_pca.coef_)\n",
    "print(lassoCV_pca.intercept_)\n",
    "lasso_pca_mse = mean_squared_error(y_test, y_pred_lassoCV_pca)\n",
    "lasso_pca_r2 = r2_score(y_test, y_pred_lassoCV_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression Initial Data</td>\n",
       "      <td>98.184059</td>\n",
       "      <td>-32.973723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression PCA</td>\n",
       "      <td>4.880006</td>\n",
       "      <td>-0.688584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Standardized Initial Data</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso PCA</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         regressor        MSE         R2\n",
       "0   Linear Regression Initial Data  98.184059 -32.973723\n",
       "1            Linear Regression PCA   4.880006  -0.688584\n",
       "2  Lasso Standardized Initial Data   4.524136  -0.565445\n",
       "3                        Lasso PCA   4.524136  -0.565445"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'regressor':['Linear Regression Initial Data', 'Linear Regression PCA', 'Lasso Standardized Initial Data', 'Lasso PCA'],\n",
    "                          'MSE':[lreg_initial_mse, lreg_pca_mse, lasso_initial_standardized_mse, lasso_pca_mse],\n",
    "                          'R2':[lreg_initial_r2, lreg_pca_r2, lasso_initial_standardized_r2, lasso_pca_r2]})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results with Lasso are worse that expected, it zeros all the coefficients and leaves only the intercept as non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. k-NN\n",
    "#### - Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with lowest MSE: 0.03827867818058003 using cross-validation: 19.0\n"
     ]
    }
   ],
   "source": [
    "n = np.linspace(1,20,20)\n",
    "mse_knn = []\n",
    "mse_knn1 = []\n",
    "\n",
    "for ni in n:\n",
    "    knn = KNeighborsRegressor(n_neighbors=int(ni))#.fit(X_train1, y_train)\n",
    "    #y_test_knn = knn.predict(X_test1)\n",
    "    mse_knn11 = cross_validate(knn, X_train1, y_train, cv = 3)\n",
    "    mse_knn1.append(np.mean(np.abs(mse_knn11['test_score'])))\n",
    "    #mse_knn.append(mean_squared_error(y_test, y_test_knn))\n",
    "\n",
    "#print(mse_knn)\n",
    "#print(mse_knn1)\n",
    "#print(f\"NN with lowest MSE: {np.min(mse_knn)} on test set: {n[np.argmin(mse_knn)]}\")\n",
    "print(f\"NN with lowest MSE: {np.min(mse_knn1)} using cross-validation: {n[np.argmin(mse_knn1)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8220317174515355 0.02351843686797994\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=19).fit(X_train1, y_train)\n",
    "y_test_knn = knn.predict(X_test1)\n",
    "knn_initial_standardized_mse = mean_squared_error(y_test, y_test_knn)\n",
    "knn_initial_strandardized_r2 = r2_score(y_test, y_test_knn)\n",
    "print(knn_initial_standardized_mse, knn_initial_strandardized_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with lowest MSE: 0.21933953419314864 using cross-validation: 21.0\n"
     ]
    }
   ],
   "source": [
    "n = np.linspace(1,30,30)\n",
    "mse_knn = []\n",
    "mse_knn1 = []\n",
    "\n",
    "for ni in n:\n",
    "    knn = KNeighborsRegressor(n_neighbors=int(ni))#.fit(X_train, y_train)\n",
    "    #y_test_knn = knn.predict(X_test)\n",
    "    mse_knn11 = cross_validate(knn, X_train, y_train, cv = 10)\n",
    "    mse_knn1.append(np.mean(np.abs(mse_knn11['test_score'])))\n",
    "    #mse_knn.append(mean_squared_error(y_test, y_test_knn))\n",
    "\n",
    "#print(f\"NN with lowest MSE: {np.min(mse_knn)} on test set: {n[np.argmin(mse_knn)]}\")\n",
    "print(f\"NN with lowest MSE: {np.min(mse_knn1)} using cross-validation: {n[np.argmin(mse_knn1)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.878066439909288 -0.34189150169871674\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=21).fit(X_train, y_train)\n",
    "y_test_knn1 = knn.predict(X_test)\n",
    "knn_initial_standardized_mse1 = mean_squared_error(y_test, y_test_knn1)\n",
    "knn_initial_strandardized_r21 = r2_score(y_test, y_test_knn1)\n",
    "print(knn_initial_standardized_mse1, knn_initial_strandardized_r21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with lowest MSE: 2.3917000000000006 on test set: 6.0\n",
      "NN with lowest MSE: 0.03249975381702286 using cross-validation: 20.0\n"
     ]
    }
   ],
   "source": [
    "n = np.linspace(1,20,20)\n",
    "mse_knn_pca = []\n",
    "mse_knn1_pca = []\n",
    "\n",
    "for ni in n:\n",
    "    knn_pca = KNeighborsRegressor(n_neighbors=int(ni)).fit(X_train_pca, y_train)\n",
    "    y_test_knn_pca = knn_pca.predict(X_test_pca)\n",
    "    mse_knn11_pca = cross_validate(knn_pca, X_train_pca, y_train, cv = 3)\n",
    "    mse_knn1_pca.append(np.mean(np.abs(mse_knn11_pca['test_score'])))\n",
    "    mse_knn_pca.append(mean_squared_error(y_test, y_test_knn_pca))\n",
    "\n",
    "print(f\"NN with lowest MSE: {np.min(mse_knn_pca)} on test set: {n[np.argmin(mse_knn_pca)]}\")\n",
    "print(f\"NN with lowest MSE: {np.min(mse_knn1_pca)} using cross-validation: {n[np.argmin(mse_knn1_pca)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pca = KNeighborsRegressor(n_neighbors=19).fit(X_train_pca, y_train)\n",
    "y_test_knn_pca = knn_pca.predict(X_test_pca)\n",
    "knn_pca_mse = mean_squared_error(y_test, y_test_knn_pca)\n",
    "knn_pca_r2 = r2_score(y_test, y_test_knn_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression Initial Data</td>\n",
       "      <td>98.184059</td>\n",
       "      <td>-32.973723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression PCA</td>\n",
       "      <td>4.880006</td>\n",
       "      <td>-0.688584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Standardized Initial Data</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso PCA</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kNN Standardized Initial Data</td>\n",
       "      <td>2.822032</td>\n",
       "      <td>0.023518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kNN PCA Data</td>\n",
       "      <td>5.455055</td>\n",
       "      <td>-0.887562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         regressor        MSE         R2\n",
       "0   Linear Regression Initial Data  98.184059 -32.973723\n",
       "1            Linear Regression PCA   4.880006  -0.688584\n",
       "2  Lasso Standardized Initial Data   4.524136  -0.565445\n",
       "3                        Lasso PCA   4.524136  -0.565445\n",
       "4    kNN Standardized Initial Data   2.822032   0.023518\n",
       "5                     kNN PCA Data   5.455055  -0.887562"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'regressor':['Linear Regression Initial Data', 'Linear Regression PCA', 'Lasso Standardized Initial Data', 'Lasso PCA','kNN Standardized Initial Data', 'kNN PCA Data'],\n",
    "                          'MSE':[lreg_initial_mse, lreg_pca_mse, lasso_initial_standardized_mse, lasso_pca_mse, knn_initial_standardized_mse, knn_pca_mse],\n",
    "                          'R2':[lreg_initial_r2, lreg_pca_r2, lasso_initial_standardized_r2, lasso_pca_r2, knn_initial_strandardized_r2, knn_pca_r2]})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DecisionTreeRegressor\n",
    "#### - Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest MSE: 0.5230325794015717, with max_tree depth: 1.0\n",
      "The lowest test MSE: 0.1060050661157033, with max_tree depth: 3.0\n"
     ]
    }
   ],
   "source": [
    "max_depths = np.linspace(1,20,20)\n",
    "mse_dtree = []\n",
    "mse_dtree_test = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dtree =  DecisionTreeRegressor(max_depth = max_depth).fit(X_train, y_train)\n",
    "    mse_dtree1 = cross_validate(dtree, X_train, y_train, cv = 3)\n",
    "    mse_dtree.append(np.mean(np.abs(mse_dtree1['test_score'])))\n",
    "    y_test_pred_dtree = dtree.predict(X_test)\n",
    "    mse_dtree_test.append(mean_squared_error(y_test,y_test_pred_dtree))\n",
    "\n",
    "print(f\"The lowest MSE: {np.min(mse_dtree)}, with max_tree depth: {max_depths[np.argmin(mse_dtree)]}\")\n",
    "print(f\"The lowest test MSE: {np.min(mse_dtree_test)}, with max_tree depth: {max_depths[np.argmin(mse_dtree_test)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.29705882 48.29705882]\n",
      "[47.4 50.8]\n",
      "-0.22308461345050867\n",
      "3.5347145328719667\n"
     ]
    }
   ],
   "source": [
    "dtree_best = DecisionTreeRegressor(max_depth = 1).fit(X_train, y_train)\n",
    "y_test_pred_best = dtree_best.predict(X_test)\n",
    "print(dtree_best.predict(X_test))\n",
    "print(y_test)\n",
    "print(r2_score(y_test, y_test_pred_best))\n",
    "print(mean_squared_error(y_test, y_test_pred_best))\n",
    "\n",
    "dtree_initial_mse = mean_squared_error(y_test, y_test_pred_best)\n",
    "dtree_initial_r2 = r2_score(y_test, y_test_pred_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest MSE: 0.07256452909892437, with max_tree depth: 1.0\n",
      "The lowest test MSE: 2.9084640138408284, with max_tree depth: 5.0\n"
     ]
    }
   ],
   "source": [
    "max_depths = np.linspace(1,20,20)\n",
    "mse_dtree_pca = []\n",
    "mse_dtree_test_pca = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dtree_pca =  DecisionTreeRegressor(max_depth = max_depth).fit(X_train_pca, y_train)\n",
    "    mse_dtree1_pca = cross_validate(dtree_pca, X_train_pca, y_train, cv = 3)\n",
    "    mse_dtree_pca.append(np.mean(np.abs(mse_dtree1_pca['test_score'])))\n",
    "    y_test_pred_dtree_pca = dtree_pca.predict(X_test_pca)\n",
    "    mse_dtree_test_pca.append(mean_squared_error(y_test,y_test_pred_dtree_pca))\n",
    "\n",
    "print(f\"The lowest MSE: {np.min(mse_dtree_pca)}, with max_tree depth: {max_depths[np.argmin(mse_dtree_pca)]}\")\n",
    "print(f\"The lowest test MSE: {np.min(mse_dtree_test_pca)}, with max_tree depth: {max_depths[np.argmin(mse_dtree_test_pca)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.56515152 48.56515152]\n",
      "[47.4 50.8]\n",
      "-0.09898370302585491\n",
      "3.176062901744718\n"
     ]
    }
   ],
   "source": [
    "dtree_pca = DecisionTreeRegressor(max_depth = 1).fit(X_train_pca, y_train)\n",
    "y_test_pred_pca = dtree_pca.predict(X_test_pca)\n",
    "print(dtree_pca.predict(X_test_pca))\n",
    "print(y_test)\n",
    "print(r2_score(y_test, y_test_pred_pca))\n",
    "print(mean_squared_error(y_test, y_test_pred_pca))\n",
    "\n",
    "dtree_pca_mse = mean_squared_error(y_test, y_test_pred_pca)\n",
    "dtree_pca_r2 = r2_score(y_test, y_test_pred_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression Initial Data</td>\n",
       "      <td>98.184059</td>\n",
       "      <td>-32.973723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression PCA</td>\n",
       "      <td>4.880006</td>\n",
       "      <td>-0.688584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Standardized Initial Data</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso PCA</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kNN Standardized Initial Data</td>\n",
       "      <td>2.822032</td>\n",
       "      <td>0.023518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kNN PCA Data</td>\n",
       "      <td>5.455055</td>\n",
       "      <td>-0.887562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree Initial Data</td>\n",
       "      <td>3.176063</td>\n",
       "      <td>-0.098984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree PCA</td>\n",
       "      <td>3.176063</td>\n",
       "      <td>-0.098984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         regressor        MSE         R2\n",
       "0   Linear Regression Initial Data  98.184059 -32.973723\n",
       "1            Linear Regression PCA   4.880006  -0.688584\n",
       "2  Lasso Standardized Initial Data   4.524136  -0.565445\n",
       "3                        Lasso PCA   4.524136  -0.565445\n",
       "4    kNN Standardized Initial Data   2.822032   0.023518\n",
       "5                     kNN PCA Data   5.455055  -0.887562\n",
       "6       Decision Tree Initial Data   3.176063  -0.098984\n",
       "7                Decision Tree PCA   3.176063  -0.098984"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'regressor':['Linear Regression Initial Data', 'Linear Regression PCA', 'Lasso Standardized Initial Data', 'Lasso PCA','kNN Standardized Initial Data', 'kNN PCA Data', 'Decision Tree Initial Data', 'Decision Tree PCA'],\n",
    "                          'MSE':[lreg_initial_mse, lreg_pca_mse, lasso_initial_standardized_mse, lasso_pca_mse, knn_initial_standardized_mse, knn_pca_mse, dtree_initial_mse, dtree_pca_mse],\n",
    "                          'R2':[lreg_initial_r2, lreg_pca_r2, lasso_initial_standardized_r2, lasso_pca_r2, knn_initial_strandardized_r2, knn_pca_r2, dtree_initial_r2, dtree_pca_r2]})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RandomForrestRegressor\n",
    "#### - Initial DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we need better hyperparameter tunning and also to determine a random state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2722395452074928, 0.41019710417999583, 0.4312511551136617, 0.48860418994631166, 0.45745119558382935, 0.46143621842110255, 0.4726748380580712, 0.48005663538186183, 0.4787613017579991, 0.47773676459565423, 0.47825809666898395, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274, 0.47817879360780274]\n"
     ]
    }
   ],
   "source": [
    "max_depths = np.linspace(1,20,20)\n",
    "\n",
    "mse_rf = []\n",
    "for max_depth in max_depths:\n",
    "    rf = RandomForestRegressor(n_estimators = 100, max_depth = max_depth, random_state = 42).fit(X_train, y_train)\n",
    "    mse_rf1 = cross_validate(rf, X_train, y_train, cv = 3)\n",
    "    mse_rf.append(np.mean(np.abs(mse_rf1['test_score'])))\n",
    "print(mse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "d = max_depths[np.argmin(mse_rf)]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8069674570412915\n",
      "[47.04455289 47.58784663]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 163, max_depth = 1, random_state = 42).fit(X_train, y_train)\n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "print(r2_score(y_test, y_test_pred_rf))\n",
    "print(y_test_pred_rf)\n",
    "\n",
    "rf_initial_mse = mean_squared_error(y_test, y_test_pred_rf)\n",
    "rf_initial_r2 = r2_score(y_test, y_test_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   57.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'n_estimators': 700}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 700, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 20, 20)]\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, scoring = 'neg_mean_squared_error', n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2721728521493502, 0.2768589920421576, 0.20466616125611256, 0.22723576989775443, 0.20346306418622842, 0.18135083230046942, 0.17462477993073264, 0.17364658750301212, 0.17295502907980662, 0.17334402283712422, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303, 0.17263287111135303]\n"
     ]
    }
   ],
   "source": [
    "max_depths = np.linspace(1,20,20)\n",
    "\n",
    "mse_rf_pca = []\n",
    "for max_depth in max_depths:\n",
    "    rf_pca = RandomForestRegressor(n_estimators = 100, max_depth = max_depth, random_state = 42).fit(X_train_pca, y_train)\n",
    "    mse_rf1_pca = cross_validate(rf_pca, X_train_pca, y_train, cv = 3)\n",
    "    mse_rf_pca.append(np.mean(np.abs(mse_rf1_pca['test_score'])))\n",
    "print(mse_rf_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "d = max_depths[np.argmin(mse_rf_pca)]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6618301038062351\n",
      "[47.717 47.717]\n"
     ]
    }
   ],
   "source": [
    "rf_pca = RandomForestRegressor(n_estimators = 100, max_depth = d, random_state = 42).fit(X_train_pca, y_train)\n",
    "y_test_pred_rf_pca = rf_pca.predict(X_test_pca)\n",
    "print(r2_score(y_test, y_test_pred_rf_pca))\n",
    "print(y_test_pred_rf_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 381 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'n_estimators': 700}"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 700, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 20, 20)]\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, scoring = 'r2', n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6241819846322127\n",
      "[46.98365808 46.92773564]\n"
     ]
    }
   ],
   "source": [
    "rf_pca = RandomForestRegressor(n_estimators = 700, max_depth = 5, random_state = 42).fit(X_train_pca, y_train)\n",
    "y_test_pred_rf_pca = rf_pca.predict(X_test_pca)\n",
    "print(r2_score(y_test, y_test_pred_rf_pca))\n",
    "print(y_test_pred_rf_pca)\n",
    "\n",
    "rf_pca_mse = mean_squared_error(y_test, y_test_pred_rf_pca)\n",
    "rf_pca_r2 = r2_score(y_test, y_test_pred_rf_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression Initial Data</td>\n",
       "      <td>98.184059</td>\n",
       "      <td>-32.973723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression PCA</td>\n",
       "      <td>4.880006</td>\n",
       "      <td>-0.688584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Standardized Initial Data</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso PCA</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kNN Standardized Initial Data</td>\n",
       "      <td>2.822032</td>\n",
       "      <td>0.023518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kNN PCA Data</td>\n",
       "      <td>5.455055</td>\n",
       "      <td>-0.887562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree Initial Data</td>\n",
       "      <td>3.176063</td>\n",
       "      <td>-0.098984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree PCA</td>\n",
       "      <td>3.176063</td>\n",
       "      <td>-0.098984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forrest Initial Data</td>\n",
       "      <td>5.222136</td>\n",
       "      <td>-0.806967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forrest PCA</td>\n",
       "      <td>7.583886</td>\n",
       "      <td>-1.624182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         regressor        MSE         R2\n",
       "0   Linear Regression Initial Data  98.184059 -32.973723\n",
       "1            Linear Regression PCA   4.880006  -0.688584\n",
       "2  Lasso Standardized Initial Data   4.524136  -0.565445\n",
       "3                        Lasso PCA   4.524136  -0.565445\n",
       "4    kNN Standardized Initial Data   2.822032   0.023518\n",
       "5                     kNN PCA Data   5.455055  -0.887562\n",
       "6       Decision Tree Initial Data   3.176063  -0.098984\n",
       "7                Decision Tree PCA   3.176063  -0.098984\n",
       "8      Random Forrest Initial Data   5.222136  -0.806967\n",
       "9               Random Forrest PCA   7.583886  -1.624182"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'regressor':['Linear Regression Initial Data', 'Linear Regression PCA', 'Lasso Standardized Initial Data', 'Lasso PCA','kNN Standardized Initial Data', 'kNN PCA Data', 'Decision Tree Initial Data', 'Decision Tree PCA', 'Random Forrest Initial Data', 'Random Forrest PCA'],\n",
    "                          'MSE':[lreg_initial_mse, lreg_pca_mse, lasso_initial_standardized_mse, lasso_pca_mse, knn_initial_standardized_mse, knn_pca_mse, dtree_initial_mse, dtree_pca_mse, rf_initial_mse, rf_pca_mse],\n",
    "                          'R2':[lreg_initial_r2, lreg_pca_r2, lasso_initial_standardized_r2, lasso_pca_r2, knn_initial_strandardized_r2, knn_pca_r2, dtree_initial_r2, dtree_pca_r2, rf_initial_r2, rf_pca_r2]})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Boosting\n",
    "#### - Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   12.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 88}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   15.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 88}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   20.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 165}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 191}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 165}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 320}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 372}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 449}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 397}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   22.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 397}\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 11, stop = 501, num = 20)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "\n",
    "scores = []\n",
    "params = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = depth), random_state = 0)\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2, scoring = 'r2')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    params.append(grid_search.best_params_)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5502864282968083\n",
      "[47.03333333 49.23      ]\n",
      "[47.4 50.8]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "depth = 5\n",
    "n_estimators = 165\n",
    "\n",
    "ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = depth), n_estimators = n_estimators, learning_rate = learning_rate, random_state = 0).fit(X_train, y_train)\n",
    "y = ada.predict(X_test)\n",
    "print(r2_score(y_test, y))\n",
    "print(y)\n",
    "print(y_test)\n",
    "\n",
    "ada_initial_mse = mean_squared_error(y_test, y)\n",
    "ada_initial_r2 = r2_score(y_test, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - PCA DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   10.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'n_estimators': 11}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   15.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'n_estimators': 320}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 11}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 294}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 165}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 191}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 294}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 165}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 217}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   23.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 217}\n"
     ]
    }
   ],
   "source": [
    "depths = np.linspace(1,10,10)\n",
    "\n",
    "# Base Estimator\n",
    "#base_estimator = [DecisionTreeRegressor(max_depth = depth) for depth in depths]\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 11, stop = 501, num = 20)]\n",
    "# Learning Rate\n",
    "learning_rate = [1e-3,1e-2,1e-1,1,10]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate}\n",
    "\n",
    "scores = []\n",
    "params = []\n",
    "for depth in depths:\n",
    "    # Create a based model\n",
    "    ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = depth), random_state = 0)\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = ada, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2, scoring = 'r2')\n",
    "    grid_search.fit(X_train_pca, y_train)\n",
    "    \n",
    "    params.append(grid_search.best_params_)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09616839451773118\n",
      "[48.5728125 48.5728125]\n",
      "[47.4 50.8]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "depth = 1\n",
    "n_estimators = 11\n",
    "\n",
    "ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = depth), n_estimators = n_estimators, learning_rate = learning_rate, random_state = 0).fit(X_train_pca, y_train)\n",
    "y_pca = ada.predict(X_test_pca)\n",
    "print(r2_score(y_test, y_pca))\n",
    "print(y_pca)\n",
    "print(y_test)\n",
    "\n",
    "ada_pca_mse = mean_squared_error(y_test, y_pca)\n",
    "ada_pca_r2 = r2_score(y_test, y_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*performance rather poor here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression Initial Data</td>\n",
       "      <td>98.184059</td>\n",
       "      <td>-32.973723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression PCA</td>\n",
       "      <td>4.880006</td>\n",
       "      <td>-0.688584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Standardized Initial Data</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso PCA</td>\n",
       "      <td>4.524136</td>\n",
       "      <td>-0.565445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kNN Standardized Initial Data</td>\n",
       "      <td>2.822032</td>\n",
       "      <td>0.023518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kNN PCA Data</td>\n",
       "      <td>5.455055</td>\n",
       "      <td>-0.887562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree Initial Data</td>\n",
       "      <td>3.176063</td>\n",
       "      <td>-0.098984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree PCA</td>\n",
       "      <td>3.176063</td>\n",
       "      <td>-0.098984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forrest Initial Data</td>\n",
       "      <td>5.222136</td>\n",
       "      <td>-0.806967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forrest PCA</td>\n",
       "      <td>7.583886</td>\n",
       "      <td>-1.624182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Boosting Initial Data</td>\n",
       "      <td>1.299672</td>\n",
       "      <td>0.550286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Boosting PCA</td>\n",
       "      <td>3.167927</td>\n",
       "      <td>-0.096168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          regressor        MSE         R2\n",
       "0    Linear Regression Initial Data  98.184059 -32.973723\n",
       "1             Linear Regression PCA   4.880006  -0.688584\n",
       "2   Lasso Standardized Initial Data   4.524136  -0.565445\n",
       "3                         Lasso PCA   4.524136  -0.565445\n",
       "4     kNN Standardized Initial Data   2.822032   0.023518\n",
       "5                      kNN PCA Data   5.455055  -0.887562\n",
       "6        Decision Tree Initial Data   3.176063  -0.098984\n",
       "7                 Decision Tree PCA   3.176063  -0.098984\n",
       "8       Random Forrest Initial Data   5.222136  -0.806967\n",
       "9                Random Forrest PCA   7.583886  -1.624182\n",
       "10            Boosting Initial Data   1.299672   0.550286\n",
       "11                     Boosting PCA   3.167927  -0.096168"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'regressor':['Linear Regression Initial Data', 'Linear Regression PCA', 'Lasso Standardized Initial Data', 'Lasso PCA','kNN Standardized Initial Data', 'kNN PCA Data', 'Decision Tree Initial Data', 'Decision Tree PCA', 'Random Forrest Initial Data', 'Random Forrest PCA', 'Boosting Initial Data', 'Boosting PCA'],\n",
    "                          'MSE':[lreg_initial_mse, lreg_pca_mse, lasso_initial_standardized_mse, lasso_pca_mse, knn_initial_standardized_mse, knn_pca_mse, dtree_initial_mse, dtree_pca_mse, rf_initial_mse, rf_pca_mse, ada_initial_mse, ada_pca_mse],\n",
    "                          'R2':[lreg_initial_r2, lreg_pca_r2, lasso_initial_standardized_r2, lasso_pca_r2, knn_initial_strandardized_r2, knn_pca_r2, dtree_initial_r2, dtree_pca_r2, rf_initial_r2, rf_pca_r2, ada_initial_r2, ada_pca_r2]})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
